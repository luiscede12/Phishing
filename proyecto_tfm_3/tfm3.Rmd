
# Librerías

```{r}
library(tidymodels)
library(tidyverse)
library(outliers)
library(skimr)
library(ggthemes)
library(corrr)
library(themis)
library(knitr)
library(kableExtra)
library(corrr)
library(corrplot)
library(moments)
library(pscl)
library(MASS)
library(ranger)
library(ggfortify)
library(knitr)
library(vip)
library(rpart.plot)
library(MASS)
library(caret)
library(Boruta)
library(MXM)
library(LiblineaR)
library(kernlab)
library(xgboost)
library(baguette)
library(pROC)
library(lightgbm)
library(MLmetrics)
```

# Datos

Cargamos los datos brutos.

```{r}
phishing_bruto <- 
  read.csv("dataset_full.csv")
```

# Muestreo

comentar

```{r}
phishing_sample <-
  phishing_bruto |> group_by(phishing) |> 
  slice_sample(prop = 0.2) |> 
  ungroup()
phishing_sample |> count(phishing) |> mutate(porc = 100*n/sum(n))
```

# Fase 3: modificación (fuera de la receta)

Con lo observado en la fase de exploración deberemos tomar dos tipos decisiones:

* Las que afectan a la **base de datos en general**: pasar a factores, problemas de 
codificación o rango, variables que no aportan, creación de variables en general, etc

* Las que afectan a un **algoritmo en concreto**: normalización para la métrica, 
recategorización, tratamiento de outliers/ausentes, dummyficación, etc.

Primero procedemos a las **modificaciones estructurales**:

Como habíamos observado, había determinadas variables que estaban codificadas 
como numéricas cuando realmente eran de carácter cualitativo. De este modo, 
nuestro primer paso será convertir las variables domain_in_ip, server_client_domain, 
tld_present_params, email_in_url, domain_spf, tls_ssl_certificate, url_google_index, 
domain_google_index, url_shortened y nuestra variable objetivo phishing.

Por otro lado, eliminaremos de la memoria el dataset con todos los registros, ya que a partir de 
ahora, usaremos nuestra muestra estratificada para nuestros modelos.

```{r}
phishing_sample <-
  phishing_sample |> mutate(domain_in_ip = forcats::as_factor(domain_in_ip),
                           email_in_url = forcats::as_factor(email_in_url),
                           domain_spf = forcats::as_factor(domain_spf),
                           tls_ssl_certificate = forcats::as_factor(tls_ssl_certificate),
                           url_google_index = forcats::as_factor(url_google_index),
                           domain_google_index = forcats::as_factor(domain_google_index),
                           url_shortened = forcats::as_factor(url_shortened),
                           phishing = forcats::as_factor(phishing),
                           tld_present_params = forcats::as_factor(tld_present_params))
```

# Fase 3: modificación (dentro de la receta)

## Partición

Antes de comenzar a elaborar nuestra "receta", la cual contendrá todas las instrucciones 
de procesamiento de datos que estarán enfocados en un determinado algoritmo, comenzaremos 
realizando nuestra partición. Para ello comenzaremos dividiendo nuestros datos en train 
y test, con un 70% en entrenamiento y un 30% en este último.

```{r}
set.seed(12345)
phishing_split <- initial_split(phishing_sample, strata = phishing, prop = 0.75)
phishing_split
```

Con el argumento strata le hemos indicado que la partición se haga de forma 
proporcional en función de nuestra variable objetivo, para de este modo tener una 
proporción similar de ambos tipos de webs en cada conjunto generado.

En hoteles split tenemos las instrucciones de la partición, ahora vamos a aplicarlas:

```{r}
set.seed(12345)
phishing_train <- training(phishing_split)
phishing_test <- testing(phishing_split)
```

Tras ello nunca está de más comprobar que efectivamente está hecho de forma 
estratificada

```{r}
# train
phishing_train |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))

# test
phishing_test |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))
```

Como vemos, se mantienen las proporciones originales para ambos niveles de la 
variable objetivo.

## Validación

Por último crearemos un conjunto de validación, para de esta forma poder obtener 
métricas de la calidad de los modelos que probemos sin ser aplicado directamente 
al conjunto de test. Esto nos permitirá hacer una selección previa de el mejor 
modelo que consideremos aplicar al conjunto de prueba y realizar modelos en 
creciente complejidad.

Para ello, vamos a usar un 30% del 75% de los datos que tenemos en entrenamiento. 
Por lo tanto, nos quedría un 47,5% de los datos en entrenamiento, un 22,5% en 
validación y el 30% de test el cual no va a ser alterado.

```{r}
set.seed(12345)
phishing_val <- validation_split(phishing_train, strata = phishing, prop = 0.7)
phishing_val
```

Le hemos especificado a la función que use un 70% de entrenamiento y que el resto 
lo use para crear el conjunto de validación. Una vez más, le especificamos que 
separe este conjunto de forma estratificada.

## Receta split

```{r}
# RECETA
tree_rec <- 
 recipe(data = phishing_sample, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params, email_in_url, url_google_index, url_shortened, domain_google_index)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)}))
```

bake

```{r}
phishing_sample2 <- 
  bake(tree_rec |> prep(), new_data = NULL)
```

Apaño de nombres para el modelo

```{r}
nombres <- phishing_sample2 |> dplyr::select(where(is.factor)) |> names()
phishing_sample2 <- phishing_sample2 |> 
  mutate(across(all_of(nombres), ~recode(., "-1" = "Nav", "1" = "Yes", "0" = "No")))
```


## Partición

Antes de comenzar a elaborar nuestra "receta", la cual contendrá todas las instrucciones 
de procesamiento de datos que estarán enfocados en un determinado algoritmo, comenzaremos 
realizando nuestra partición. Para ello comenzaremos dividiendo nuestros datos en train 
y test, con un 70% en entrenamiento y un 30% en este último.

```{r}
set.seed(12345)
phishing_split2 <- initial_split(phishing_sample2, strata = phishing, prop = 0.75)
phishing_split2
```

Con el argumento strata le hemos indicado que la partición se haga de forma 
proporcional en función de nuestra variable objetivo, para de este modo tener una 
proporción similar de ambos tipos de webs en cada conjunto generado.

En hoteles split tenemos las instrucciones de la partición, ahora vamos a aplicarlas:

```{r}
set.seed(12345)
phishing_train2 <- training(phishing_split2)
phishing_test2 <- testing(phishing_split2)
```

Tras ello nunca está de más comprobar que efectivamente está hecho de forma 
estratificada

```{r}
# train
phishing_train2 |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))

# test
phishing_test2 |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))
```

Val

```{r}
set.seed(12345)
phishing_val2 <- validation_split(phishing_train2, strata = phishing, prop = 0.7)
phishing_val2
```

# GBM

# gbm

Modelos

```{r}
set.seed(12345)


gbmgrid <- expand.grid(shrinkage = c(0.2, 0.1, 0.05, 0.03, 0.01, 0.001),
 n.minobsinnode = c(5, 10, 20),
 n.trees = c(100, 500, 1000, 5000),
 interaction.depth=c(2))

control<-trainControl(method = "cv", number = 10, savePredictions = "all",
 classProbs = TRUE, repeats = 5, summaryFunction = twoClassSummary) 

library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

gbm_model <- train(phishing ~.,
 method = "gbm",trControl = control,tuneGrid = gbmgrid,
 data = phishing_train2, distribution = "bernoulli", bag.fraction = 1, verbose = FALSE, metric = "ROC")

stopCluster(make_cluster)
registerDoSEQ()

 
plot(gbm_model)
```

reejecutamos
 
```{r}
gbmgrid <- expand.grid(shrinkage = c(0.05),
 n.minobsinnode = c(10),
 n.trees = c(4000, 5000, 6000),
 interaction.depth=c(2))

library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

gbm_model <- train(phishing ~.,
 method = "gbm",trControl = control,tuneGrid = gbmgrid,
 data = phishing_train2, distribution = "bernoulli", bag.fraction = 1, verbose = FALSE, metric = "ROC")

stopCluster(make_cluster)
registerDoSEQ()

gbm_model
 
plot(gbm_model)
```

## Modelo final

```{r}

library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()
set.seed(12345)
gbmgrid <- expand.grid(shrinkage = c(0.05),
 n.minobsinnode = c(10),
 n.trees = c(5000),
 interaction.depth=c(2))

gbm_model_final <- train(phishing ~.,
 method = "gbm",trControl = control,tuneGrid = gbmgrid,
 data = phishing_train2, distribution = "bernoulli", bag.fraction = 1, verbose = FALSE, metric = "ROC")
gbm_model_final

stopCluster(make_cluster)
registerDoSEQ()

library(pROC)

# Extracción de las predicciones y las observaciones
predictions <- gbm_model_final$pred$Yes
observations <- ifelse(gbm_model_final$pred$obs == "Yes", 1, 0)

# Cálculo del AUC
auc <- roc(observations, predictions)$auc
auc
gbm_model_final$results


sensitivity <- sum(predictions[observations == 1] >= 0.5) / sum(observations == 1)
sensitivity

specificity <- sum(predictions[observations == 0] < 0.5) / sum(observations == 0)
specificity


var_auc <- var(roc(observations, predictions)$specificities)
var_auc
var_sensitivity <- var(predictions[observations == 1] >= 0.5)
var_specificity <- var(predictions[observations == 0] < 0.5)


total_observations <- length(observations)
total_positive_observations <- sum(observations == 1)
total_negative_observations <- sum(observations == 0)
accuracy <- (sensitivity * total_positive_observations + specificity * total_negative_observations) / total_observations
accuracy
```

Predicciones

```{r}
gbm_pred <- predict(gbm_model_final, phishing_test2)
gbm_pred

confusionMatrix(gbm_pred, reference = phishing_test2$phishing, positive = "Yes")

gbm_probs <- predict(gbm_model_final, newdata = phishing_test2, type = "prob")[, 1]
roc_obj <- roc(response = phishing_test2$phishing, gbm_probs)
plot(roc_obj, main = "Curva ROC", xlab = "Tasa de Falsos Positivos", ylab = "Tasa de Verdaderos Positivos")
auc <- auc(roc_obj)
auc
```

## Importancia var

```{r}
summary(gbm_model_final)
tabla<-summary(gbm_model_final)
par(cex=1.5,las=2)
barplot(tabla$rel.inf,names.arg=row.names(tabla))
tabla <- tabla |> slice_head(n = 7)
barplot(tabla$rel.inf,names.arg=row.names(tabla), cex.names = 0.7)
tabla

tabla <- tabla[order(-tabla$rel.inf), ]


ggplot(data = tabla, aes(x = reorder(row.names(tabla), -rel.inf), y = rel.inf)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  geom_text(aes(label = round(rel.inf, 2)), vjust = -0.3, size = 3, color = "black") +
  labs(x = NULL, y = "Relative Importance") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 10),
        axis.title.y = element_text(size = 12, vjust = 0.5),
        plot.title = element_text(size = 14, face = "bold"))

```


## Cruzada box

```{r}
phishing_train_box <- as.data.frame(phishing_train2)
source("cruzadas ensamblado binaria fuente.R")

media_gbm<-cruzadagbmbin(data=phishing_train_box, vardep="phishing", listconti=c("qty_tld_url", "length_url", "qty_vowels_domain", "domain_length", 
"directory_length", "file_length", "params_length", "time_response", 
"asn_ip", "time_domain_activation", "time_domain_expiration", 
"qty_ip_resolved", "qty_nameservers", "qty_mx_servers", "ttl_hostname", 
"qty_redirects", "total_dots", "total_hyphens", "total_underlines", 
"total_commas", "total_asterisks", "total_slashes", "total_equals", 
"total_ats", "total_ands", "total_spaces", "total_dollars", "total_percents", 
"total_exclamations", "total_questionmarks", "total_pluses", 
"total_tildes"), listclass=c("domain_in_ip", "email_in_url", "domain_spf", "tls_ssl_certificate", 
"url_google_index", "domain_google_index", "url_shortened", 
"tld_present_parameters", "directory_char_avb", "params_char_avb", 
"file_char_avb", "time_domain_activation_avb", "time_domain_expiration_avb", 
"domain_spf_avb", "domain_google_index_avb", "time_response_avb", 
"asn_ip_avb", "qty_ip_resolved_avb", "qty_redirects_avb", "tld_present_parameters_avb", 
"params_length_avb", "file_length_avb", "directory_length_avb"
), grupos=10,sinicio=1234,repe=5, n.minobsinnode=10,shrinkage=0.2,n.trees=1000,interaction.depth=2)


media_gbm$modelo = "gbm"
data <- media_gbm[[1]][["auc"]]
data <- as.data.frame(data)
data
par(cex.axis=0.8)
boxplot(data=data, data, main="AUC", col = "lightblue")
sd(data$data)
```

# REC XGB

```{r}
# RECETA
rec_xgb <- 
 recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # TRY
  step_mutate(directory_char_not_avb = rowSums(across(contains("_directory"), ~ . == -1))) |>
  step_mutate(params_char_not_avb = rowSums(across(contains("_params"), ~ . == -1))) |>
  step_mutate(file_char_not_avb = rowSums(across(contains("_file"), ~ . == -1))) |>
  step_mutate(directory_char_avb = forcats::as_factor(ifelse(directory_char_not_avb == 17, 0, 1)),
              params_char_avb = forcats::as_factor(ifelse(params_char_not_avb == 17, 0, 1)),
              file_char_avb = forcats::as_factor(ifelse(file_char_not_avb == 17, 0, 1))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  step_rm(c(directory_char_not_avb, params_char_not_avb, file_char_not_avb)) |> 
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              domain_google_index_avb = forcats::as_factor(ifelse(domain_google_index == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Eliminamos las categorías con <100 obs recategorizando niveles con el nivel modal
  step_mutate(url_google_index = forcats::fct_collapse(url_google_index, 
                                              "0" = c("0", "-1"),
                                              "1" = "1")) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |>
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)}))
```

bake

```{r}
xgb_prep <- bake(rec_xgb |> prep(), new_data = NULL)
xgb_prep |> glimpse()
```


# XGboost

```{r}
xgboost_model <- 
  boost_tree(mtry = tune("n_pred"),
             trees = 1000, min_n = tune("min_n"), learn_rate = tune("learn_rate"),
             loss_reduction = tune("loss_reduction")) |> 
  set_engine("xgboost") |> set_mode("classification")

grid_xgboost <- 
  expand_grid("n_pred" = seq(2, 22, 4),
              "min_n" = c(10, 50, 100, 300, 1000),
              "learn_rate" = c(0.0001, 0.001, 0.01, 1),
              "loss_reduction" = c(0.0001, 0.001, 0.01, 1)) 
grid_xgboost

phishing_xgboost_wflow <-
  workflow() |> 
  add_recipe(rec_xgb) |> 
  add_model(xgboost_model)

phishing_cv_folds <-
  vfold_cv(data = phishing_train, v = 10, repeats = 5, strata = phishing)
```

Paralelizamos

```{r}
library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

metricas <-
  metric_set(yardstick::accuracy, yardstick::sensitivity, yardstick::specificity, yardstick::roc_auc)
xgboost_tune_par <- 
  phishing_xgboost_wflow |> 
  tune_grid(resamples = phishing_cv_folds,
            grid = grid_xgboost, metrics = metricas,
            control =
              control_grid(verbose = TRUE, allow_par = TRUE, save_pred = TRUE))

stopCluster(make_cluster)
registerDoSEQ()
```


```{r}
metric_df <- xgboost_tune_par |> collect_metrics()
metric_df
```

```{r}
xgboost_tune_par |> select_best("roc_auc")
best_xgboost <- xgboost_tune_par |> select_best("roc_auc")
best_xgboost
```

```{r}
final_xgb_wflow <- phishing_xgboost_wflow |> finalize_workflow(best_xgboost)
final_xgb_wflow
```

final fit

```{r}
set.seed(12345)
final_xgb_fit <-
  final_xgb_wflow |> last_fit(phishing_split, metrics = metric_set(yardstick::accuracy, yardstick::sensitivity,
                                     yardstick::specificity, yardstick::roc_auc))
final_xgb_fit |> collect_metrics()
```

## Predicción

```{r}
predict(extract_workflow(final_xgb_fit), phishing_test)
predict(extract_workflow(final_xgb_fit), phishing_test, type = "prob")
# Inlcuimos predicciones en la tabla
prob_test_xgb <-
  augment(extract_workflow(final_xgb_fit), phishing_test)

# Matriz de confusión
conf_mat_rf <-
  prob_test_xgb |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6")
conf_mat_rf
```

## BOX

```{r}
# Obtener las métricas AUC para cada repetición y conjunto de validación
auc_results_xgb <- xgboost_tune_par |> 
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  dplyr::select(.metric, .estimator, mean, .config)

auc_values_xgb <- unlist(auc_results_xgb$mean)

# Crear un dataframe con los valores de AUC combinados
df_xgb <- data.frame(AUC = auc_values_xgb)

# Crear el gráfico de caja
ggplot(df_xgb, aes(x = "", y = AUC)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "AUC en conjuntos de validación",
       x = "",
       y = "AUC") +
  theme_minimal()

library(glue)
glue("la varianza del modelo XGBoost es {round(sd(auc_results_xgb$mean), 5)}")
```

# VisualPred

```{r}
source("funcion resultadosglm.R")
source("funcion resultadosnnet.R")
source("funcion resultadosrf.R")
source("funcion resultadossvm.R")
source("funcion resultadosgbm.R")
source("funcion seleccionar 2.0.R")
source("toydata 2.0.R")

require(egg)
library(visualpred)
library(caret)
listconti <- xgb_prep |> dplyr::select(where(is.numeric)) |> names()
xgbprep <- xgb_prep |> dplyr::select(-phishing)
listclass <- xgbprep |> dplyr::select(where(is.factor)) |> names()
vardep <- "phishing"
xgb_prep <- xgb_prep |> mutate(phishing = as_factor(ifelse(phishing == 1, "Yes", "No")))
xgb_prep <- as.data.frame(xgb_prep)
result<-famdcontour(dataf=xgb_prep,listconti=listconti, listclass = "", vardep=vardep,
                    title="nnet",title2=" ",selec=0,modelo="nnet", nodos = 15, decay = 0.1, maxit = 1500)
result[[1]]
result[[2]]
result[[3]]
result[[4]]
result[[5]]
result[[6]]

# RECETA
rec_visual <- 
 recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # TRY
  step_mutate(directory_char_not_avb = rowSums(across(contains("_directory"), ~ . == -1))) |>
  step_mutate(params_char_not_avb = rowSums(across(contains("_params"), ~ . == -1))) |>
  step_mutate(file_char_not_avb = rowSums(across(contains("_file"), ~ . == -1))) |>
  step_mutate(directory_char_avb = forcats::as_factor(ifelse(directory_char_not_avb == 17, 0, 1)),
              params_char_avb = forcats::as_factor(ifelse(params_char_not_avb == 17, 0, 1)),
              file_char_avb = forcats::as_factor(ifelse(file_char_not_avb == 17, 0, 1))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  step_rm(c(directory_char_not_avb, params_char_not_avb, file_char_not_avb)) |> 
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              domain_google_index_avb = forcats::as_factor(ifelse(domain_google_index == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Eliminamos las categorías con <100 obs recategorizando niveles con el nivel modal
  step_mutate(url_google_index = forcats::fct_collapse(url_google_index, 
                                              "0" = c("0", "-1"),
                                              "1" = "1")) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |>
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)}))

visual_data <- bake(rec_visual |> prep(), new_data = NULL)
listconti <- visual_data |> dplyr::select(where(is.numeric)) |> names()
visualdata <- visual_data |> dplyr::select(-phishing)
listclass <- visualdata |> dplyr::select(where(is.factor)) |> names()
visual_data <- visual_data |> as.data.frame()

result<-famdcontour(dataf=visual_data,listconti=listconti,listclass=listclass,vardep=vardep,
                    title="Random Forest",title2=" ",selec=0,modelo="rf",classvar=0,
                    sampsize=400,mtry=20)
result[[1]]
result[[2]]
```


# Ensamblado

## Receta split2

```{r}
# RECETA
ens_rec <- 
 recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params, email_in_url, url_google_index, url_shortened, domain_google_index)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Tratamiento de outliers
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "mad")) > 3 & 
                                                             skewness(x) > 3, NA, x)})) |> 
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "z")) > 2.5 & 
                                                              skewness(x) <= 3, NA, x)})) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)}))
  
```


```{r}
phishing_sample3 <- 
  bake(ens_rec |> prep(), new_data = NULL)
nombres <- phishing_sample3 |> dplyr::select(where(is.factor)) |> names()
phishing_sample3 <- phishing_sample3 |> 
  mutate(across(all_of(nombres), ~recode(., "-1" = "Nav", "1" = "Yes", "0" = "No")))
set.seed(12345)
phishing_split3 <- initial_split(phishing_sample3, strata = phishing, prop = 0.75)
phishing_split3
phishing_train3 <- training(phishing_split3)
phishing_test3 <- testing(phishing_split3)
phishing_val3 <- validation_split(phishing_train3, strata = phishing, prop = 0.7)
phishing_val3
```




```{r}
source("cruzadas ensamblado binaria fuente.R")
vardep<-"phishing"
listconti<-c("qty_tld_url" , "length_url" , "domain_length" , 
    "directory_length" , "file_length" , "params_length" , "asn_ip" , 
    "time_domain_activation" , "time_domain_expiration" , "qty_nameservers" , 
    "qty_mx_servers" , "ttl_hostname" , "qty_redirects" , "total_dots" , 
    "total_hyphens" , "total_underlines" , "total_commas" , "total_asterisks" , 
    "total_slashes" , "total_equals" , "total_ats" , "total_ands" , "total_percents" , 
    "total_pluses" , "total_tildes" , "domain_in_ip_X1" , "domain_spf_X0" , 
    "domain_spf_X1" , "tls_ssl_certificate_X1" , "url_shortened_X1" , 
    "tld_present_parameters_X0" , "time_domain_activation_avb_X1" , 
    "time_domain_expiration_avb_X1" , "asn_ip_avb_X1" , "qty_ip_resolved_avb_X1" , 
    "qty_redirects_avb_X1" , "directory_length_avb_X1")
listclass<-c("")
grupos <- 10
sinicio <- 1234
repe <- 5
phishing_train3 <- as.data.frame(phishing_train3)

```

## Cruzadas para ensamblar

```{r}
library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

medias1<-cruzadalogistica(data=phishing_train3,
                          vardep=vardep,listconti=listconti,
                          listclass=listclass,grupos=grupos,sinicio=sinicio,repe=repe)

medias1bis<-as.data.frame(medias1[1])
medias1bis$modelo<-"Logistica"
predi1<-as.data.frame(medias1[2])
predi1$logi<-predi1$Yes

medias2<-cruzadaavnnetbin(data=phishing_train3,
                          vardep=vardep,listconti=listconti,
                          listclass=listclass,grupos=grupos,sinicio=sinicio,repe=repe,
                          size=c(22),decay=c(0.1),repeticiones=5,itera=1000)

medias2bis<-as.data.frame(medias2[1])
medias2bis$modelo<-"avnnet"
predi2<-as.data.frame(medias2[2])
predi2$avnnet<-predi2$Yes


medias3<-cruzadarfbin(data=phishing_train3,
                      vardep=vardep,listconti=listconti,
                      listclass=listclass,grupos=grupos,sinicio=sinicio,repe=repe,
                      mtry=14,ntree=1000,nodesize=1,replace=TRUE)


medias3bis<-as.data.frame(medias3[1])
medias3bis$modelo<-"rf"
predi3<-as.data.frame(medias3[2])
predi3$rf<-predi3$Yes

medias4<-cruzadagbmbin(data=phishing_train3,
                       vardep=vardep,listconti=listconti,
                       listclass=listclass,grupos=grupos,sinicio=sinicio,repe=repe,
                       n.minobsinnode=5,shrinkage=0.001,n.trees=1000,interaction.depth=2)

medias4bis<-as.data.frame(medias4[1])
medias4bis$modelo<-"gbm"
predi4<-as.data.frame(medias4[2])
predi4$gbm<-predi4$Yes

medias5<-cruzadaxgbmbin(data=phishing_train3,
                        vardep=vardep,listconti=listconti,
                        listclass=listclass,grupos=grupos,sinicio=sinicio,repe=repe,
                        min_child_weight=10,eta=0.08,nrounds=1000,max_depth=6,
                        gamma=0,colsample_bytree=1,subsample=1,
                        alpha=0,lambda=0)


medias5bis<-as.data.frame(medias5[1])
medias5bis$modelo<-"xgbm"
predi5<-as.data.frame(medias5[2])
predi5$xgbm<-predi5$Yes


medias6<-cruzadaSVMbinRBF(data=phishing_train3,
                          vardep=vardep,listconti=listconti,
                          listclass=listclass,grupos=grupos,
                          sinicio=sinicio,repe=repe,
                          C=0.84,sigma=0.016)

medias6bis<-as.data.frame(medias6[1])
medias6bis$modelo<-"svmRadial"
predi6<-as.data.frame(medias6[2])
predi6$svmRadial<-predi6$Yes

union1<-rbind(medias1bis,medias2bis,
              medias3bis,medias4bis,medias5bis,medias6bis)

stopCluster(make_cluster)
registerDoSEQ()

par(cex.axis=0.8)
boxplot(data=union1,tasa~modelo,col="lightblue",main='TASA FALLOS')
boxplot(data=union1,auc~modelo,col="lightblue",main='AUC')

```

## Consutrucción de ensamblados

```{r}
# CONSTRUCCIÓN DE TODOS LOS ENSAMBLADOS
# SE UTILIZARÁN LOS ARCHIVOS SURGIDOS DE LAS FUNCIONES LLAMADOS predi1,...

unipredi<-cbind(predi1,predi2,predi3,predi4,predi5,predi6)

unipredi<- unipredi[, !duplicated(colnames(unipredi))]

unipredi$predi7 <- (unipredi$logi + unipredi$avnnet) / 2
unipredi$predi8 <- (unipredi$logi + unipredi$rf) / 2
unipredi$predi9 <- (unipredi$logi + unipredi$gbm) / 2
unipredi$predi10 <- (unipredi$logi + unipredi$xgbm) / 2
unipredi$predi11 <- (unipredi$logi + unipredi$svmRadial) / 2
unipredi$predi12 <- (unipredi$avnnet + unipredi$rf) / 2
unipredi$predi13 <- (unipredi$avnnet + unipredi$gbm) / 2
unipredi$predi14 <- (unipredi$avnnet + unipredi$xgbm) / 2
unipredi$predi15 <- (unipredi$avnnet + unipredi$svmRadial) / 2
unipredi$predi16 <- (unipredi$rf + unipredi$gbm) / 2
unipredi$predi17 <- (unipredi$rf + unipredi$xgbm) / 2
unipredi$predi18 <- (unipredi$rf + unipredi$svmRadial) / 2
unipredi$predi19 <- (unipredi$gbm + unipredi$xgbm) / 2
unipredi$predi20 <- (unipredi$gbm + unipredi$svmRadial) / 2
unipredi$predi21 <- (unipredi$xgbm + unipredi$svmRadial) / 2
unipredi$predi22 <- (unipredi$logi + unipredi$avnnet + unipredi$rf) / 3
unipredi$predi23 <- (unipredi$logi + unipredi$avnnet + unipredi$gbm) / 3
unipredi$predi24 <- (unipredi$logi + unipredi$avnnet + unipredi$xgbm) / 3
unipredi$predi25 <- (unipredi$logi + unipredi$avnnet + unipredi$svmRadial) / 3
unipredi$predi26 <- (unipredi$logi + unipredi$rf + unipredi$gbm) / 3
unipredi$predi27 <- (unipredi$logi + unipredi$rf + unipredi$xgbm) / 3
unipredi$predi28 <- (unipredi$logi + unipredi$rf + unipredi$svmRadial) / 3
unipredi$predi29 <- (unipredi$logi + unipredi$gbm + unipredi$xgbm) / 3
unipredi$predi30 <- (unipredi$logi + unipredi$gbm + unipredi$svmRadial) / 3
unipredi$predi31 <- (unipredi$xgbm + unipredi$svmRadial) / 3
unipredi$predi32 <- (unipredi$avnnet + unipredi$rf + unipredi$gbm) / 3
unipredi$predi33 <- (unipredi$avnnet + unipredi$rf + unipredi$xgbm) / 3
unipredi$predi34 <- (unipredi$avnnet + unipredi$rf + unipredi$svmRadial) / 3
unipredi$predi35 <- (unipredi$avnnet + unipredi$gbm + unipredi$xgbm) / 3
unipredi$predi36 <- (unipredi$avnnet + unipredi$gbm + unipredi$svmRadial) / 3
unipredi$predi37 <- (unipredi$avnnet + unipredi$xgbm + unipredi$svmRadial) / 3
unipredi$predi38 <- (unipredi$rf + unipredi$gbm + unipredi$xgbm) / 3
unipredi$predi39 <- (unipredi$rf + unipredi$gbm + unipredi$svmRadial) / 3
unipredi$predi40 <- (unipredi$rf + unipredi$xgbm + unipredi$svmRadial) / 3
unipredi$predi41 <- (unipredi$logi + unipredi$avnnet + unipredi$rf + unipredi$gbm) / 4
unipredi$predi42 <- (unipredi$logi + unipredi$avnnet + unipredi$rf + unipredi$xgbm) / 4
unipredi$predi43 <- (unipredi$logi + unipredi$avnnet + unipredi$rf + unipredi$svmRadial) / 4
unipredi$predi44 <- (unipredi$logi + unipredi$gbm + unipredi$xgbm + unipredi$svmRadial) / 4
unipredi$predi45 <- (unipredi$avnnet + unipredi$rf + unipredi$gbm + unipredi$xgbm) / 4
unipredi$predi46 <- (unipredi$avnnet + unipredi$rf + unipredi$gbm + unipredi$svmRadial) / 4
unipredi$predi47 <- (unipredi$logi + unipredi$avnnet + unipredi$rf + unipredi$xgbm + unipredi$svmRadial) / 5
unipredi$predi48 <- (unipredi$avnnet + unipredi$rf + unipredi$gbm + unipredi$xgbm + unipredi$svmRadial) / 5

```

listado de modelos a considerar

```{r}
dput(names(unipredi))
listado <- c( "rf", "gbm","xgbm", "logi", "avnnet", "svmRadial", "predi7", "predi8", "predi9","predi10", "predi11", "predi12", "predi13", "predi14", 
"predi15", "predi16", "predi17", "predi18", "predi19", "predi20", 
"predi21", "predi22", "predi23", "predi24", "predi25", "predi26", 
"predi27", "predi28", "predi29", "predi30", "predi31", "predi32", 
"predi33", "predi34", "predi35", "predi36", "predi37", "predi38", 
"predi39", "predi40", "predi41", "predi42", "predi43", "predi44",
"predi45", "predi46", "predi47", "predi48")
```

## Funciones y medias

```{r}
# Defino funcion tasafallos

tasafallos<-function(x,y) {
  confu<-confusionMatrix(x,y)
  tasa<-confu[[3]][1]
  return(tasa)
}

auc<-function(x,y) {
  curvaroc<-roc(response=x,predictor=y)
  auc<-curvaroc$auc
  return(auc)
}

# Se obtiene el numero de repeticiones CV y se calculan las medias por repe en
# el data frame medias0

repeticiones<-nlevels(factor(unipredi$Rep))
unipredi$Rep<-as.factor(unipredi$Rep)
unipredi$Rep<-as.numeric(unipredi$Rep)


medias0<-data.frame(c())
for (prediccion in listado)
{
  unipredi$proba<-unipredi[,prediccion]
  unipredi[,prediccion]<-ifelse(unipredi[,prediccion]>0.5,"Yes","No")
  for (repe in 1:repeticiones)
  {
    paso <- unipredi[(unipredi$Rep==repe),]
    pre<-factor(paso[,prediccion])
    archi<-paso[,c("proba","obs")]
    archi<-archi[order(archi$proba),]
    obs<-paso[,c("obs")]
    tasa=1-tasafallos(pre,obs)
    t<-as.data.frame(tasa)
    t$modelo<-prediccion
    auc<-suppressMessages(auc(archi$obs,archi$proba))
    t$auc<-auc
    medias0<-rbind(medias0,t)
  }
}
```

## Boxplots

```{r}
# Finalmente boxplot

par(cex.axis=0.5,las=2)
boxplot(data=medias0,tasa~modelo,col="lightblue",main="TASA FALLOS")

# Para AUC se utiliza la variable auc del archivo medias0

boxplot(data=medias0,auc~modelo,col="lightblue",main="AUC")

# PRESENTACION TABLA MEDIAS

library(dplyr)
tablamedias<-medias0 %>%
  group_by(modelo) %>%
  summarize(tasa=mean(tasa))     

tablamedias<-as.data.frame(tablamedias[order(tablamedias$tasa),])
tablamedias

# ORDENACIÓN DEL FACTOR MODELO POR LAS MEDIAS EN TASA
# PARA EL GRAFICO

medias0$modelo <- with(medias0,
                       reorder(modelo,tasa, mean))
par(cex.axis=0.7,las=2)
boxplot(data=medias0,tasa~modelo,col="lightblue", main='TASA FALLOS')

```

### AUC

```{r}
# PRESENTACION TABLA MEDIAS

tablamedias2<-medias0 %>%
  group_by(modelo) %>%
  summarize(auc=mean(auc))     

tablamedias2<-tablamedias2[order(-tablamedias2$auc),]
tablamedias2


# ORDENACIÓN DEL FACTOR MODELO POR LAS MEDIAS EN AUC
# PARA EL GRAFICO

medias0$modelo <- with(medias0,
                       reorder(modelo,auc, mean))
par(cex.axis=0.7,las=2)
boxplot(data=medias0,auc~modelo,col="lightblue", main='AUC')
```

### Gráficos de apoyo

```{r}
# Esto es para eliminar columnas duplicadas
unipredi<- unipredi[, !duplicated(colnames(unipredi))]

# Me quedo con la primera repetición de validación cruzada para los análisis
# A veces hay que poner Rep1 y otras veces Rep01, depende de cuantas repeticiones hemos pedido

unigraf<-unipredi[unipredi$Rep=="Rep01",]
# Correlaciones entre predicciones de cada algoritmo individual
solos<-c("logi", "avnnet",
         "rf","gbm",  "xgbm",
         "svmRadial")
mat<-unigraf[,solos]
matrizcorr<-cor(mat)
matrizcorr
library(corrplot)
corrplot(matrizcorr, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45,cl.lim=c(0.7,1),is.corr=FALSE)

library(ggplot2)

qplot(predi38, rf,data=unigraf,colour=obs)+
  geom_hline(yintercept=0.5, color="black", size=1)+
  geom_vline(xintercept=0.5, color="black", size=1)+
  theme_minimal()

qplot(predi17,avnnet,data=unigraf,colour=obs)+
  geom_hline(yintercept=0.5, color="black", size=1)+
  geom_vline(xintercept=0.5, color="black", size=1)+
  theme_minimal()

qplot(svmRadial,avnnet,data=unigraf,colour=obs)+
  geom_hline(yintercept=0.5, color="black", size=1)+
  geom_vline(xintercept=0.5, color="black", size=1)+
  theme_minimal()

```

## Contrastes de hipótesis

1º Comparación

```{r}
listamodelos<-c("logi","predi17")

datacontraste<-medias0[which(medias0$modelo%in%listamodelos),]

# Para Tasa de fallos

res <- t.test(datacontraste$tasa ~datacontraste$modelo)
res

# Para auc

res <- t.test(datacontraste$auc ~datacontraste$modelo)
res
```

2º Comparación

```{r}
listamodelos<-c("logi","gbm")

datacontraste<-medias0[which(medias0$modelo%in%listamodelos),]

# Para Tasa de fallos

res <- t.test(datacontraste$tasa ~datacontraste$modelo)
res

# Para auc

res <- t.test(datacontraste$auc ~datacontraste$modelo)
res
```

# Logit 2

## Modelo

Definimos el modelo y el flujo de trabajo en el que se ajustará

```{r}
set.seed(12345)
# Construimos modelo
log_reg <- logistic_reg() |> set_engine("glm")
# Construimos flujo
logit_flow <-
  workflow() |> 
  add_model(log_reg) |> 
  add_recipe(logist_rec)
# Ajuste
logit_fit <- logit_flow |> fit(data = phishing_train)
```

## Repetimos la receta

```{r}
# RECETA
logist_rec <- 
  recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              domain_google_index_avb = forcats::as_factor(ifelse(domain_google_index == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Eliminamos las categorías con <100 obs recategorizando niveles con el nivel modal
  step_mutate(url_google_index = forcats::fct_collapse(url_google_index, 
                                              "0" = c("0", "-1"),
                                              "1" = "1")) |> 
  # Roles
  add_role(where(is.factor), new_role = "qual") |> 
  add_role(where(is.numeric), new_role = "quant") |> 
  # Tratamiento de outliers
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "mad")) > 3 & 
                                                             skewness(x) > 3, NA, x)})) |> 
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "z")) > 2.5 & 
                                                              skewness(x) <= 3, NA, x)})) |> 
  # Imputamos los outliers convetidos a NA
  step_impute_knn(has_role("quant")) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)})) |> 
  # Seleccionamos solo las variables selecciondas
  step_select("qty_tld_url", "file_length", "params_length", "asn_ip", "time_domain_activation", 
"qty_nameservers", "total_slashes", "total_ats", "domain_in_ip_X1", 
"domain_spf_X0", "url_shortened_X1", "time_domain_activation_avb_X1", 
"qty_redirects_avb_X1", "directory_length_avb_X1", "phishing")
```

corr

```{r}
corr_check <- bake(logist_rec |> prep(), new_data = NULL) |> dplyr::select(-phishing)
```

## Correlación

```{r}
cor_matrix <- 
  corr_check |> dplyr::select(where(is.numeric)) |> cor() |> round(2)
cor_matrix |> corrplot(method = "number", tl.cex = 0.5, number.cex = 0.5, type = "lower")
```

## Flujo y ajuste final

```{r}
set.seed(12345)
# Construimos flujo
logit_flow <-
  workflow() |> add_model(log_reg) |> add_recipe(logist_rec)
# Ajuste
final_logit <- logit_flow |> fit(data = phishing_train)
```



Eval

```{r}
exp(coef(final_logit |> extract_fit_engine()))
final_logit |> extract_fit_engine() |> summary()
glance(final_logit)
pR2(final_logit |> extract_fit_engine())
tidy(final_logit)
```


Predicciones y odds

```{r}
predicciones_logist <-
  augment(final_logit, new_data = phishing_test) |> 
  mutate(odds = .pred_1 / .pred_0, log.odds = log(odds))
predicciones_logist |> conf_mat(phishing, .pred_class)
auc_roc <- predicciones_logist |> roc_auc(truth = phishing, .pred_0)
auc_roc

conf_mat_logit <-
  predicciones_logist |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6") +
  theme_minimal()
conf_mat_logit

roc_par_1 <- 
  predicciones_logist |> roc_curve(truth = phishing, .pred_0 )
roc_par_1 |> autoplot() +
   ggtitle("Curva ROC") +
  theme_stata()
```

# Cruzada red boxplot

```{r}
phishing_train_box <- as.data.frame(phishing_train2)
source("cruzadas ensamblado binaria fuente.R")

media_avnet<-cruzadaavnnetbin(data=phishing_train_box, vardep="phishing", listconti=c("qty_tld_url", "length_url", "qty_vowels_domain", "domain_length", 
"directory_length", "file_length", "params_length", "time_response", 
"asn_ip", "time_domain_activation", "time_domain_expiration", 
"qty_ip_resolved", "qty_nameservers", "qty_mx_servers", "ttl_hostname", 
"qty_redirects", "total_dots", "total_hyphens", "total_underlines", 
"total_commas", "total_asterisks", "total_slashes", "total_equals", 
"total_ats", "total_ands", "total_spaces", "total_dollars", "total_percents", 
"total_exclamations", "total_questionmarks", "total_pluses", 
"total_tildes"), listclass=c("domain_in_ip", "email_in_url", "domain_spf", "tls_ssl_certificate", 
"url_google_index", "domain_google_index", "url_shortened", 
"tld_present_parameters", "directory_char_avb", "params_char_avb", 
"file_char_avb", "time_domain_activation_avb", "time_domain_expiration_avb", 
"domain_spf_avb", "domain_google_index_avb", "time_response_avb", 
"asn_ip_avb", "qty_ip_resolved_avb", "qty_redirects_avb", "tld_present_parameters_avb", 
"params_length_avb", "file_length_avb", "directory_length_avb"
), grupos=10,sinicio=1234,repe=5)


media_gbm$modelo = "gbm"
data <- media_gbm[[1]][["auc"]]
data <- as.data.frame(data)
data
par(cex.axis=0.8)
boxplot(data=data, data, main="AUC", col = "lightblue")
sd(data$data)
```


# BOXPLOTS

```{r}
xgb_box <- data.frame(modelo = c("XGBoost"), auc = df_xgb$AUC)
gbm_box <- data.frame(modelo = c("GBM"), auc = data$data)
tree_box <- data.frame(modelo = c("decision_tree"), auc = arbol$"0.8909035")
rf_box <- data.frame(modelo = c("rf"), auc = random$"0.9663643")
svm_box <- data.frame(modelo = c("svm"), auc = svm$"0.9671756")
combined_box <- rbind(xgb_box, gbm_box, tree_box, rf_box, svm_box)
rf_box <- rf_box |> mutate(auc = as.numeric(auc))
tree_box <- tree_box |> mutate(auc = as.numeric(auc))
svm_box <- svm_box |> mutate(auc = as.numeric(auc))
red_box <- read.csv("medias5.csv")
logit_box <- read.csv("medias2.csv")

a <- ggplot(xgb_box, aes(x = auc)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  coord_flip() +
  scale_y_continuous(labels = NULL) +
  ylab("XGBoost")

b <- ggplot(gbm_box, aes(x = auc)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  coord_flip() +
  scale_y_continuous(labels = NULL) +
  ylab("GBM")

c <- ggplot(rf_box, aes(x = auc)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  coord_flip() +
  scale_y_continuous(labels = NULL) +
  ylab("Random forest")

d <- ggplot(svm_box, aes(x = auc)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  coord_flip() +
  scale_y_continuous(labels = NULL) +
  ylab("SVM radial")

e <- ggplot(red_box, aes(x = auc)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  coord_flip() +
  scale_y_continuous(labels = NULL) +
  ylab("Red Neuronal")

f <- ggplot(logit_box, aes(x = auc)) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  coord_flip() +
  scale_y_continuous(labels = NULL) +
  ylab("Logit")

grid.arrange(a,b,c)
grid.arrange(d,e,f)

library(glue)
glue("la varianza de XGBoost es {round(sd(xgb_box$auc), 5)}")
glue("la varianza de rf es {round(sd(rf_box$auc), 5)}")
glue("la varianza de svm es {round(sd(svm_box$auc), 5)}")
glue("la varianza de logit es {round(sd(logit_box$auc), 5)}")
glue("la varianza de red es {round(sd(red_box$auc), 5)}")
glue("la varianza de gbm es {round(sd(gbm_box$auc), 5)}")
```


