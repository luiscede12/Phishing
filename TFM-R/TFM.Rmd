---
title: "Técnicas de machine learning para la detección de páginas web de phishing"
description: |
  Trabajo de fin de máster
author:
  - name: Luis Pérez García (DNI 32724256-V)
    affiliation: Universidad Complutense de Madrid
    affiliation_url: 
date: "`r Sys.Date()`"
output:
    distill::distill_article:
        highlight: kate
        colorlinks: true
        code_folding: false
        toc: true            
        toc_depth: 3     
---

```{r setup, include = FALSE}
# Ajuste comunes de los chunk
knitr::opts_chunk$set(fig.width = 9, fig.asp = 1, out.width = "100%",
                      message = FALSE, warning = FALSE,
                      echo = TRUE, res = 400)
```

# Objetivo

Nuestro objetivo es detectar a través de diferentes técnias de **machine learning**, aquellas 
páginas web que tratan de llevar a cabo la técnica de engaño "phishing".

# Librerías

Cargamos la librerías que usaremos a lo largo de todo el trabajo.

```{r}
library(tidymodels)
library(tidyverse)
library(outliers)
library(skimr)
library(ggthemes)
library(corrr)
library(themis)
library(knitr)
library(kableExtra)
library(corrr)
library(corrplot)
library(moments)
library(pscl)
library(MASS)
library(ranger)
library(ggfortify)
library(knitr)
library(vip)
library(rpart.plot)
library(MASS)
library(caret)
library(Boruta)
library(MXM)
library(LiblineaR)
library(kernlab)
library(xgboost)
source("funcion steprepetido binaria.R")
source("cruzadas avnnet y log binaria.R")
```

# Datos

Cargamos los datos brutos.

```{r}
phishing_bruto <- 
  read.csv("dataset_full.csv")
```

Transformamos nuestra objetivo a factor (ver análisis exploratorio inicial)

```{r}
phishing_bruto <-
  phishing_bruto |> mutate(phishing = as_factor(phishing))
```


# Fases 1-2-3 de Muestreo, Exploración y Modificación

Comenzamos con las 3 primeras fases de la **metodología SEMMA**.

# Muestreo

comentar

```{r}
phishing_sample <-
  phishing_bruto |> group_by(phishing) |> 
  slice_sample(prop = 0.2) |> 
  ungroup()
phishing_sample |> count(phishing) |> mutate(porc = 100*n/sum(n))
```


# Análisis de colinealidad?¿


```{r}
library(corrr)
cor_matrix <- phishing_bruto |> select(where(is.numeric)) |> cor() |> round(2)
cor_matrix
```

Comentario



# Fase 3: modificación (fuera de la receta)

Con lo observado en la fase de exploración deberemos tomar dos tipos decisiones:

* Las que afectan a la **base de datos en general**: pasar a factores, problemas de 
codificación o rango, variables que no aportan, creación de variables en general, etc

* Las que afectan a un **algoritmo en concreto**: normalización para la métrica, 
recategorización, tratamiento de outliers/ausentes, dummyficación, etc.

Primero procedemos a las **modificaciones estructurales**:

Como habíamos observado, había determinadas variables que estaban codificadas 
como numéricas cuando realmente eran de carácter cualitativo. De este modo, 
nuestro primer paso será convertir las variables domain_in_ip, server_client_domain, 
tld_present_params, email_in_url, domain_spf, tls_ssl_certificate, url_google_index, 
domain_google_index, url_shortened y nuestra variable objetivo phishing.

Por otro lado, eliminaremos de la memoria el dataset con todos los registros, ya que a partir de 
ahora, usaremos nuestra muestra estratificada para nuestros modelos.

```{r}
phishing_sample <-
  phishing_sample |> mutate(domain_in_ip = forcats::as_factor(domain_in_ip),
                           email_in_url = forcats::as_factor(email_in_url),
                           domain_spf = forcats::as_factor(domain_spf),
                           tls_ssl_certificate = forcats::as_factor(tls_ssl_certificate),
                           url_google_index = forcats::as_factor(url_google_index),
                           domain_google_index = forcats::as_factor(domain_google_index),
                           url_shortened = forcats::as_factor(url_shortened),
                           phishing = forcats::as_factor(phishing),
                           tld_present_params = forcats::as_factor(tld_present_params))
```

Por el resto, no tenemos ningúna modificación adicional que debemos de aplicar 
a nuestra base de datos, ya que todo lo demás que hemos observado anteriormente 
tendrá que ser aplicado en nuestro preprocesamiento de los algoritmos en concreto. 
Es decir, a parte de esta corrección de codificación, nuestra base de datos no presenta 
ningún defecto adicional, las modificaciones posteriores serán para intentar mejorar 
los modelos y no para corregir nuestra base de datos.

# Fase 3: modificación (dentro de la receta)

## Partición

Antes de comenzar a elaborar nuestra "receta", la cual contendrá todas las instrucciones 
de procesamiento de datos que estarán enfocados en un determinado algoritmo, comenzaremos 
realizando nuestra partición. Para ello comenzaremos dividiendo nuestros datos en train 
y test, con un 70% en entrenamiento y un 30% en este último.

```{r}
set.seed(12345)
phishing_split <- initial_split(phishing_sample, strata = phishing, prop = 0.75)
phishing_split
```

Con el argumento strata le hemos indicado que la partición se haga de forma 
proporcional en función de nuestra variable objetivo, para de este modo tener una 
proporción similar de ambos tipos de webs en cada conjunto generado.

En hoteles split tenemos las instrucciones de la partición, ahora vamos a aplicarlas:

```{r}
set.seed(12345)
phishing_train <- training(phishing_split)
phishing_test <- testing(phishing_split)
```

Tras ello nunca está de más comprobar que efectivamente está hecho de forma 
estratificada

```{r}
# train
phishing_train |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))

# test
phishing_test |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))
```

Como vemos, se mantienen las proporciones originales para ambos niveles de la 
variable objetivo.

## Validación

Por último crearemos un conjunto de validación, para de esta forma poder obtener 
métricas de la calidad de los modelos que probemos sin ser aplicado directamente 
al conjunto de test. Esto nos permitirá hacer una selección previa de el mejor 
modelo que consideremos aplicar al conjunto de prueba y realizar modelos en 
creciente complejidad.

Para ello, vamos a usar un 30% del 75% de los datos que tenemos en entrenamiento. 
Por lo tanto, nos quedría un 47,5% de los datos en entrenamiento, un 22,5% en 
validación y el 30% de test el cual no va a ser alterado.

```{r}
set.seed(12345)
phishing_val <- validation_split(phishing_train, strata = phishing, prop = 0.7)
phishing_val
```

Le hemos especificado a la función que use un 70% de entrenamiento y que el resto 
lo use para crear el conjunto de validación. Una vez más, le especificamos que 
separe este conjunto de forma estratificada.

# Regresión logística

Cálculo de la asimetría de las variables numéricas:

```{r}
for(i in not_rm_num_vars) {
  print(paste("La asimetría de la variable", i, "es:", round(skewness(phishing_bruto[,i]), 2)))
}
```



## Receta: regresión logística

```{r}
# RECETA
logist_rec <- 
  recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              domain_google_index_avb = forcats::as_factor(ifelse(domain_google_index == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Eliminamos las categorías con <100 obs recategorizando niveles con el nivel modal
  step_mutate(url_google_index = forcats::fct_collapse(url_google_index, 
                                              "0" = c("0", "-1"),
                                              "1" = "1")) |> 
  # Roles
  add_role(where(is.factor), new_role = "qual") |> 
  add_role(where(is.numeric), new_role = "quant") |> 
  # Tratamiento de outliers
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "mad")) > 3 & 
                                                             skewness(x) > 3, NA, x)})) |> 
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "z")) > 2.5 & 
                                                              skewness(x) <= 3, NA, x)})) |> 
  # Imputamos los outliers convetidos a NA
  step_impute_knn(has_role("quant")) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)}))
```

Bake

```{r}
phishing_prep <- bake(logist_rec |> prep(), new_data = NULL)
bake(logist_rec |> prep(), new_data = NULL) |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))
phishing_prep |> glimpse()
```

## Modelo

Definimos el modelo y el flujo de trabajo en el que se ajustará

```{r}
set.seed(12345)
# Construimos modelo
log_reg <- logistic_reg() |> set_engine("glm")
# Construimos flujo
logit_flow <-
  workflow() |> 
  add_model(log_reg) |> 
  add_recipe(logist_rec)
# Ajuste
logit_fit <- logit_flow |> fit(data = phishing_train)
```

Exponencial de los coeficientes

```{r}
exp(coef(logit_fit |> extract_fit_engine()))
```

Las variables con un Bj positivo implica que sus coef. exponenciales son mayores que 1, así que por 
cada incremento unitario de estas variables, se produce un incremento en la probabilidad de tener un 
problema cardiaco frente a no tenerlo (son los llamados factores de riesgo).

Aquellos que son menores que 0 implica que sus coeficientes exponenciales son menores que 1, así que 
por cada incremento unitario de estas variables, se produce un decrecimiento en la probabilidad de 
tener un problema cardiaco frente a no tenerlo.

Para evaluar nuestra regresión logística  debemos introducir el concepto de anomalía o deviance (D): 
es una una generalización del SSR (suma de residuos al cuadrado) del modelo lineal, definido como la 
diferencia entre la log-verosimilitud de mi modelo saturado (un modelo sobreajustado, que acertará 
siempre cada punto) y la log-verosimilitud de mi ajuste.

De forma sencilla, D será lo que le falta a nuestro modelo para ser «perfecto» (en el sentido de 
saturado/sobreajustado, el modelo más probable dada una muestra).

```{r}
logit_fit |> extract_fit_engine() |> summary()
```

A través de los p-valores vemos que muchas de nuestras variables carecen de significación.

```{r}
glance(logit_fit)
```

Con la función glance podemos ver varios parámetros indicativos de la calidad de nuestro ajuste.

* **null deviance**: aquella obtenida al comparar el modelo perfecto frente a un modelo sin parámetros.
En este caso, una null deviance de 60303 indica que la varianza total en la variable respuesta no 
explicada por el modelo es muy alta. Es decir, el modelo propuesto tiene una capacidad limitada para 
explicar la variabilidad en los datos.
* **logLik**: Medida de la máxima verosimilitud logarítmica del modelo. En este caso, un logLik de 
-7287 nos indica que la probabilidad de que los datos observados sean generados por el modelo de 
regresión logística ajustado es muy baja. En general, se prefiere que el log-likelihood sea lo más 
alto posible, lo que indica que el modelo se ajusta bien a los datos observados.

También podemos extraer el pseudo-R2, conocido como el coeficiente de McFadden.

```{r}
pR2(logit_fit |> extract_fit_engine())
```

..

```{r}
tidy(logit_fit)
```

...

```{r}
set.seed(12345)
reg_logit_glm <-
  glm(data = phishing_prep, phishing ~ .,
      family = "binomial")
```

Eejecutamos el step_aic indicandole como K el logaritmo del número de filas del conjunto de train, 
para que sea un cirterio BIC. Si no le indicamos un argumento adicional, lo hará con el proceso 
stepwise, la cual es una combinación del método forward y backward.

## BIC

```{r}
set.seed(12345)
library(parallel)
library(doParallel)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

# BIC
phishing_fit_BIC <- stepAIC(reg_logit_glm, k = log(nrow(phishing_train)))

stopCluster(make_cluster)
registerDoSEQ()
```

## AIC

```{r}
phishing_fit_AIC <- stepAIC(reg_logit_glm, k = 2)
```


```{r}
tidy(phishing_fit_BIC)
exp(coef(phishing_fit_BIC))
summary(phishing_fit_BIC)
summary((phishing_fit_AIC))
```

## Repetimos la receta

```{r}
# RECETA
logist_rec <- 
  recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              domain_google_index_avb = forcats::as_factor(ifelse(domain_google_index == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Eliminamos las categorías con <100 obs recategorizando niveles con el nivel modal
  step_mutate(url_google_index = forcats::fct_collapse(url_google_index, 
                                              "0" = c("0", "-1"),
                                              "1" = "1")) |> 
  # Roles
  add_role(where(is.factor), new_role = "qual") |> 
  add_role(where(is.numeric), new_role = "quant") |> 
  # Tratamiento de outliers
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "mad")) > 3 & 
                                                             skewness(x) > 3, NA, x)})) |> 
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "z")) > 2.5 & 
                                                              skewness(x) <= 3, NA, x)})) |> 
  # Imputamos los outliers convetidos a NA
  step_impute_knn(has_role("quant")) |> 
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)})) |> 
  # Seleccionamos solo las variables selecciondas
  step_select("qty_tld_url", "file_length", "params_length", "asn_ip", "time_domain_activation", 
"qty_nameservers", "total_slashes", "total_ats", "domain_in_ip_X1", 
"domain_spf_X0", "url_shortened_X1", "time_domain_activation_avb_X1", 
"qty_redirects_avb_X1", "directory_length_avb_X1", "phishing")
```

bake

```{r}
corr_check <- bake(logist_rec |> prep(), new_data = NULL) |> dplyr::select(-phishing)
```

## Correlación

```{r}
cor_matrix <- 
  corr_check |> dplyr::select(where(is.numeric)) |> cor() |> round(2)
cor_matrix |> corrplot(method = "number", tl.cex = 0.5, number.cex = 0.5, type = "lower")
```

Quitamos total equals, total ands y length url

```{r}
# RECETA
logist_rec <- 
  recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(c("length_url", "domain_length", "directory_length", 
                              "file_length", "params_length")), ~ifelse(.x == -1, NA, .x))) |> 
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              domain_google_index_avb = forcats::as_factor(ifelse(domain_google_index == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Eliminamos las categorías con <100 obs recategorizando niveles con el nivel modal
  step_mutate(url_google_index = forcats::fct_collapse(url_google_index, 
                                              "0" = c("0", "-1"),
                                              "1" = "1")) |> 
  # Roles
  add_role(where(is.factor), new_role = "qual") |> 
  add_role(where(is.numeric), new_role = "quant") |> 
  # Tratamiento de outliers
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "mad")) > 3 & 
                                                             skewness(x) > 3, NA, x)})) |> 
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "z")) > 2.5 & 
                                                              skewness(x) <= 3, NA, x)})) |> 
  # Imputamos los outliers convetidos a NA
  step_impute_knn(has_role("quant")) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)})) |> 
  # Seleccionamos solo las variables selecciondas
  step_select("qty_tld_url" , "domain_length" , 
    "directory_length" , "file_length" , "params_length" , "asn_ip" , 
    "time_domain_activation" , "time_domain_expiration" , "qty_nameservers" , 
    "qty_mx_servers" , "ttl_hostname" , "qty_redirects" , "total_dots" , 
    "total_hyphens" , "total_underlines" , "total_commas" , "total_asterisks" , 
    "total_slashes" , "total_ats" , "total_percents" , 
    "total_pluses" , "total_tildes" , "domain_in_ip_X1" , "domain_spf_X0" , 
    "domain_spf_X1" , "tls_ssl_certificate_X1" , "url_shortened_X1" , 
    "tld_present_parameters_X0" , "time_domain_activation_avb_X1" , 
    "time_domain_expiration_avb_X1" , "asn_ip_avb_X1" , "qty_ip_resolved_avb_X1" , 
    "qty_redirects_avb_X1" , "directory_length_avb_X1", "phishing")
```


## Flujo y ajuste final

```{r}
set.seed(12345)
# Construimos flujo
logit_flow <-
  workflow() |> add_model(log_reg) |> add_recipe(logist_rec)
# Ajuste
phishing_fit_BIC <- logit_flow |> fit(data = phishing_train)
```

Predicciones y odds

```{r}
predicciones_logist <-
  augment(phishing_fit_BIC, new_data = phishing_test) |> 
  mutate(odds = .pred_1 / .pred_0, log.odds = log(odds))
predicciones_logist |> conf_mat(phishing, .pred_class)
```


# Árbol de decisión

## Receta árbol

Antes de lanzar nuestro modelo de árbol, modificaremos nuestra receta anterior. En este caso, no 
realizaremos tratamiento de valores atípicos ya que este algoritmo es muy resistente a este tipo de 
observaciones. Por otro lado, tampoco será necesario dummificar nuestras variables cualitativas ya 
que el random forest admite todo tipo de variables.

```{r}
# RECETA
tree_rec <- 
 recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params, email_in_url, url_google_index, url_shortened, domain_google_index)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)}))
  
```

Bake

```{r}
bake(tree_rec |> prep(), new_data = NULL)
```

Modelo con impureza de Gini

```{r}
decision_tree <-
  decision_tree(mode = "classification", tree_depth = tune("depth"),
                min_n = tune("min_n"), cost_complexity = tune("cost"))
decision_tree_gini <- decision_tree |>  set_engine("rpart")
```

Flujo de trabajo

```{r}
phishing_tree_wflow <-
  workflow() |> 
  add_recipe(tree_rec) |> 
  add_model(decision_tree_gini)
```

Especificamos los valores para los parámetros del grid. En este tipo de algoritmo, los parámetros que especificamos son 3:

* depth: especifica la profundidad máxima del árbol, es decir, el número máximo de ramas que puede tener cada nodo del árbol.
* min_n: especifica el número mínimo de observaciones que deben estar en cada nodo para continuar dividiéndolo.
* cost: especifica el costo de clasificar una observación.

```{r}
grid_tree <-
  expand_grid("depth" = c(1, 2, 3, 4, 5),
              "min_n" = c(10, 15, 20, 30, 50, 100),
              "cost" = c(0.001, 0.01, 0.1, 0.5, 1, 2))
grid_tree
```

Ajuste

```{r}
phishing_cv_folds <-
  vfold_cv(data = phishing_train, v = 10, repeats = 5, strata = phishing)
library(doParallel)
library(parallel)

clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)

metricas_val <- metric_set(yardstick::accuracy, yardstick::sensitivity, yardstick::specificity, yardstick::roc_auc)

phishing_fit_tree_gini <-
  phishing_tree_wflow |> 
  tune_grid(resamples = phishing_cv_folds,
            grid = grid_tree, metrics = metricas_val,
            control = control_grid(verbose = TRUE,
                                   allow_par = TRUE,
                                   save_pred = TRUE))
stopCluster(make_cluster)
registerDoSEQ()
phishing_fit_tree_gini 
```

Métricas

```{r}
phishing_fit_tree_gini |> collect_metrics()
phishing_fit_tree_gini |> show_best("sensitivity")
phishing_fit_tree_gini |> show_best("roc_auc")
```


```{r}
best_tree <-
  phishing_fit_tree_gini |> select_best("roc_auc")
final_tree_flow <- phishing_tree_wflow |> finalize_workflow(best_tree)
final_tree_flow
```

```{r}
set.seed(12345)
final_tree_fit <-
  final_tree_flow |> last_fit(phishing_split, metrics = metric_set(yardstick::accuracy, yardstick::sensitivity,
                                     yardstick::specificity, yardstick::roc_auc))
final_tree_fit|> collect_metrics()
```

## Predicción con árbol

```{r}
predict(extract_workflow(final_tree_fit), phishing_test)
predict(extract_workflow(final_tree_fit), phishing_test, type = "prob")
prob_test_tree <-
  augment(extract_workflow(final_tree_fit), phishing_test)

# Matriz de confusión
conf_mat_tree <-
  prob_test_tree |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") + 
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6")
conf_mat_tree
```

## Importancia de variables y visualización del árbol

```{r}
fit_gini <-
  final_tree_fit |> 
  extract_fit_engine()
fit_gini$variable.importance

vi(fit_gini)
```

Visualizamos

```{r}
fit_gini |> 
  vip() +
  labs(x = "Variables", y = "Importancia",
       title = "IMPORTANCIA DE VARIABLES") +
  theme_minimal()
```

Visualización del árbol

```{r}
final_tree_fit |> 
  extract_fit_engine() |> 
  rpart.plot(roundint = FALSE,
             extra = 3, shadow.col = "darkgray")
```

## Plot arbol

```{r}
# Obtener las métricas AUC para cada repetición y conjunto de validación
auc_results_tree <- phishing_fit_tree_gini |> 
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  dplyr::select(.metric, .estimator, mean, .config)

auc_values_tree <- unlist(auc_results_tree$mean)

# Crear un dataframe con los valores de AUC combinados
tree_df <- data.frame(AUC = auc_values_tree)

# Crear el gráfico de caja
ggplot(tree_df, aes(x = "", y = AUC)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "AUC en conjuntos de validación",
       x = "",
       y = "AUC") +
  theme_minimal()
```



# Random forest

implementaremos un random forest, en el cual le tenemos que especificar 3 argumentos:

* Mtry: número de predictores
* min_n: número minimo de observaciones para las divisiones de nodos
* trees: número de árboles que prueba

```{r}
# RECETA
rf_rec <- 
 recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # TRY
  step_mutate(directory_char_not_avb = rowSums(across(contains("_directory"), ~ . == -1))) |>
  step_mutate(params_char_not_avb = rowSums(across(contains("_params"), ~ . == -1))) |>
  step_mutate(file_char_not_avb = rowSums(across(contains("_file"), ~ . == -1))) |>
  step_mutate(directory_char_avb = forcats::as_factor(ifelse(directory_char_not_avb == 17, 0, 1)),
              params_char_avb = forcats::as_factor(ifelse(params_char_not_avb == 17, 0, 1)),
              file_char_avb = forcats::as_factor(ifelse(file_char_not_avb == 17, 0, 1))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  step_rm(c(directory_char_not_avb, params_char_not_avb, file_char_not_avb)) |> 
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              domain_google_index_avb = forcats::as_factor(ifelse(domain_google_index == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Eliminamos las categorías con <100 obs recategorizando niveles con el nivel modal
  step_mutate(url_google_index = forcats::fct_collapse(url_google_index, 
                                              "0" = c("0", "-1"),
                                              "1" = "1")) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)}))
```

Comprobamos que funciona correctamente con bake

```{r}
bake(rf_rec |> prep(), new_data = NULL) |> View()
```

Creamos el modelo

```{r}
rf_phishing <-
  rand_forest(mode = "classification", mtry = tune("n_pred"),
              min_n = tune("min_n"), trees = 500) |> 
  set_engine("ranger", num.threads = 7)
```

Grid y flujo de trabajo

```{r}
grid_rf <- 
  expand_grid("n_pred" = seq(1, 16, 1),
              "min_n" = c(1, 5, 10, 20, 50, 100, 200, 500, 1000))
grid_rf
```

flujo

```{r}
phishing_rf_wflow <-
  workflow() |> 
  add_recipe(rf_rec) |> 
  add_model(rf_phishing)
```


Vamos a lanzar 500 árboles con 144 configuraciones posibles. 72000 árboles en total.

Activamos la parelelización para realizar una validación cruzada.

```{r}
library(parallel)
library(doParallel)
phishing_cv_folds <-
  vfold_cv(data = phishing_train, v = 10, repeats = 5, strata = phishing)
detectCores()
```

Usaremos 11 de nuestros 12 núcleos disponibles.

```{r}
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

metricas <-
  metric_set(accuracy, sensitivity, specificity, roc_auc)
rf_tune_par <- 
  phishing_rf_wflow |> 
  tune_grid(resamples = phishing_cv_folds,
            grid = grid_rf, metrics = metricas,
            control =
              control_grid(verbose = TRUE, allow_par = TRUE, save_pred = TRUE))

stopCluster(make_cluster)
registerDoSEQ()
```

Obtenemos métricas en validación

```{r}
rf_tune_par |> collect_metrics()
```

miramos el mejor segun sensitividad

```{r}
rf_tune_par |> show_best("sensitivity")
rf_tune_par |> show_best("roc_auc")
best_rf <- rf_tune_par |> select_best("roc_auc")
final_rf_flow <- phishing_rf_wflow |> finalize_workflow(best_rf)
final_rf_flow
```

Ajuste final

```{r}
final_rf_fit <-
  final_rf_flow |> last_fit(phishing_split, metrics = metric_set(accuracy, sensitivity,
                                     specificity, roc_auc))
final_rf_fit |> collect_metrics()
```

## Predicción con rf

```{r}
predict(extract_workflow(final_rf_fit), phishing_test)
predict(extract_workflow(final_rf_fit), phishing_test, type = "prob")
# Inlcuimos predicciones en la tabla
prob_test_rf <-
  augment(extract_workflow(final_rf_fit), phishing_test)

# Matriz de confusión
conf_mat_rf <-
  prob_test_rf |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6")
conf_mat_rf
```

## Rf 1000 trees

```{r}
rf_phishing <-
  rand_forest(mode = "classification", mtry = tune("n_pred"),
              min_n = tune("min_n"), trees = 1000) |> 
  set_engine("ranger", num.threads = 7, importance = "impurity")

grid_rf <- 
  expand_grid("n_pred" = seq(1, 20, 1),
              "min_n" = c(1, 5, 10, 20, 50, 100, 200, 500, 1000))
grid_rf
phishing_rf_wflow <-
  workflow() |> 
  add_recipe(rf_rec) |> 
  add_model(rf_phishing)
```

Paralelizamos

```{r}
library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

metricas <-
  metric_set(yardstick::accuracy, yardstick::sensitivity, yardstick::specificity, yardstick::roc_auc)
rf_tune_par <- 
  phishing_rf_wflow |> 
  tune_grid(resamples = phishing_cv_folds,
            grid = grid_rf, metrics = metricas,
            control =
              control_grid(verbose = TRUE, allow_par = TRUE, save_pred = TRUE))

stopCluster(make_cluster)
registerDoSEQ()
```

Vemos las métricas

```{r}
rf_tune_par |> collect_metrics()
```

Nos quedamos con el mejor de curva roc

```{r}
rf_tune_par |> show_best("roc_auc")
best_rf <- rf_tune_par |> select_best("roc_auc")
final_rf_flow <- phishing_rf_wflow |> finalize_workflow(best_rf)
final_rf_flow
```

Ajuste final

```{r}
final_rf_fit <-
  final_rf_flow |> last_fit(phishing_split, metrics = metric_set(yardstick::accuracy, yardstick::sensitivity,
                                     yardstick::specificity, yardstick::roc_auc))
final_rf_fit |> collect_metrics()

rf_mil <- final_rf_fit |> extract_fit_engine()
final_rf_fit %>%
extract_fit_parsnip() %>%
vip(geom = "point") + 
labs(title = "Random forest variable importance") +
  theme_bw() +
  xlab("Variable") +
  ylab("Importancia")
```

Predicciones

```{r}
predict(extract_workflow(final_rf_fit), phishing_test)
predict(extract_workflow(final_rf_fit), phishing_test, type = "prob")
# Inlcuimos predicciones en la tabla
prob_test_rf <-
  augment(extract_workflow(final_rf_fit), phishing_test)

# Matriz de confusión
conf_mat_rf <-
  prob_test_rf |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6")
conf_mat_rf
```

### Plot rf

```{r}
# Obtener las métricas AUC para cada repetición y conjunto de validación
auc_results_rf <- rf_tune_par |> 
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  dplyr::select(.metric, .estimator, mean, .config)

auc_values_rf <- unlist(auc_results_rf$mean)

# Crear un dataframe con los valores de AUC combinados
df_rf <- data.frame(AUC = auc_values_rf)

# Crear el gráfico de caja
ggplot(df_rf, aes(x = "", y = AUC)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "AUC en conjuntos de validación",
       x = "",
       y = "AUC") +
  theme_minimal()
```

## Importancia de variables rf

```{r}

```

Visualizamos

```{r}
fit_rf |> 
  vip() +
  labs(x = "Importancia", y = "Variables",
       title = "IMPORTANCIA DE VARIABLES") +
  theme_minimal()
```

# Selección de variables a partir de la receta regresión logística


## Boruta

```{r}
phishing_prep2 <- phishing_prep |> mutate(phishing = as_factor(ifelse(phishing == 1, "Yes", "No")))
out.boruta <- Boruta(phishing ~ ., data = phishing_prep2)

print(out.boruta)

summary(out.boruta)

sal<-data.frame(out.boruta$finalDecision)

sal2<-sal[which(sal$out.boruta.finalDecision == "Confirmed"),,drop=FALSE]
dput(row.names(sal2))

length(dput(row.names(sal2)))

# Resultado
c("qty_tld_url", "length_url", "qty_vowels_domain", "domain_length", 
"directory_length", "file_length", "params_length", "time_response", 
"asn_ip", "time_domain_activation", "time_domain_expiration", 
"qty_ip_resolved", "qty_nameservers", "qty_mx_servers", "ttl_hostname", 
"qty_redirects", "total_dots", "total_hyphens", "total_underlines", 
"total_commas", "total_slashes", "total_equals", "total_ats", 
"total_ands", "total_percents", "domain_in_ip_X1", "email_in_url_X1", 
"domain_spf_X0", "domain_spf_X1", "tls_ssl_certificate_X1", "url_shortened_X1", 
"tld_present_parameters_X0", "tld_present_parameters_X1", "time_domain_activation_avb_X1", 
"time_domain_expiration_avb_X1", "domain_spf_avb_X1", "time_response_avb_X1", 
"asn_ip_avb_X1", "qty_ip_resolved_avb_X1", "qty_redirects_avb_X1", 
"tld_present_parameters_avb_X1", "params_length_avb_X1", "directory_length_avb_X1", 
"file_length_avb_X1")
```

## MXM

```{r}
library(MXM)
vardep <- "phishing"
mmpc2 <- MMPC(vardep, phishing_prep2, max_k = 3, hash = TRUE,
              test = "testIndLogistic")

mmpc2@selectedVars

a<-dput(names(phishing_prep2[,c(mmpc2@selectedVars)]))

length(a)

a

# Resultado
c("qty_tld_url", "file_length", "params_length", "asn_ip", "time_domain_activation", 
"qty_nameservers", "total_slashes", "total_ats", "domain_in_ip_X1", 
"domain_spf_X0", "url_shortened_X1", "time_domain_activation_avb_X1", 
"qty_redirects_avb_X1", "directory_length_avb_X1")
```

## SES

```{r}
SES1 <- SES(vardep, phishing_prep2, max_k = 3, hash = TRUE,
            test = "testIndLogistic")

SES1@selectedVars

dput(names(phishing_prep2[,c(SES1@selectedVars)]))

a<-dput(names(phishing_prep2[,c(SES1@selectedVars)]))

length(a)

a

# Resultado
c("qty_tld_url", "file_length", "params_length", "asn_ip", "time_domain_activation", 
"qty_nameservers", "total_slashes", "total_ats", "domain_in_ip_X1", 
"domain_spf_X0", "url_shortened_X1", "time_domain_activation_avb_X1", 
"qty_redirects_avb_X1", "directory_length_avb_X1")
```

## Medias

```{r}
medias1<-cruzadalogistica(data = phishing_prep2,
                    vardep="phishing",listconti = c("qty_tld_url", "length_url", "qty_vowels_domain", "domain_length", 
                    "directory_length", "file_length", "params_length", "time_response", 
                    "asn_ip", "time_domain_activation", "time_domain_expiration", 
                    "qty_ip_resolved", "qty_nameservers", "qty_mx_servers", "ttl_hostname", 
                    "qty_redirects", "total_dots", "total_hyphens", "total_underlines", 
                    "total_commas", "total_slashes", "total_equals", "total_ats", 
                    "total_ands", "total_percents", "domain_in_ip_X1", "email_in_url_X1", 
                    "domain_spf_X0", "domain_spf_X1", "tls_ssl_certificate_X1", "url_shortened_X1", 
                    "tld_present_parameters_X0", "tld_present_parameters_X1", "time_domain_activation_avb_X1", 
                    "time_domain_expiration_avb_X1", "domain_spf_avb_X1", "time_response_avb_X1", 
                    "asn_ip_avb_X1", "qty_ip_resolved_avb_X1", "qty_redirects_avb_X1", 
                    "tld_present_parameters_avb_X1", "params_length_avb_X1", "directory_length_avb_X1", 
                    "file_length_avb_X1"),
                    listclass=c(""), grupos = 10, sinicio = 1234, repe = 5)

medias1$modelo="Boruta"

medias2<-cruzadalogistica(data = phishing_prep2,
                    vardep="phishing",listconti = c("qty_tld_url", "file_length", "params_length", "asn_ip", "time_domain_activation", 
"qty_nameservers", "total_slashes", "total_ats", "domain_in_ip_X1", 
"domain_spf_X0", "url_shortened_X1", "time_domain_activation_avb_X1", 
"qty_redirects_avb_X1", "directory_length_avb_X1"),
                    listclass=c(""), grupos = 10, sinicio = 1234, repe = 5)

medias2$modelo="MXM"


medias3<-cruzadalogistica(data = phishing_prep2,
                    vardep="phishing",listconti = c("qty_tld_url", "file_length", "params_length", "asn_ip", "time_domain_activation", 
"qty_nameservers", "total_slashes", "total_ats", "domain_in_ip_X1", 
"domain_spf_X0", "url_shortened_X1", "time_domain_activation_avb_X1", 
"qty_redirects_avb_X1", "directory_length_avb_X1"),
                    listclass=c(""), grupos = 10, sinicio = 1234, repe = 5)

medias3$modelo="SES"


medias4<-cruzadalogistica(data = phishing_prep2,
                    vardep="phishing",listconti = c("qty_tld_url" , "domain_length" , "directory_length" , 
    "file_length" , "params_length" , "asn_ip" , "time_domain_activation" , 
    "qty_nameservers" , "qty_mx_servers" , "ttl_hostname" , "qty_redirects" , 
    "total_dots" , "total_hyphens" , "total_underlines" , "total_commas" , 
    "total_asterisks" , "total_slashes" , "total_equals" , "total_ats" , 
    "total_percents" , "total_pluses" , "domain_in_ip_X1" , "domain_spf_X0" , 
    "tls_ssl_certificate_X1" , "url_shortened_X1" , "time_domain_activation_avb_X1" , 
    "qty_redirects_avb_X1" , "directory_length_avb_X1"),
                    listclass=c(""), grupos = 10, sinicio = 1234, repe = 5)

medias4$modelo="BIC"

medias5<-cruzadalogistica(data = phishing_prep2,
                    vardep="phishing",listconti = c("qty_tld_url" , "length_url" , "domain_length" , 
    "directory_length" , "file_length" , "params_length" , "asn_ip" , 
    "time_domain_activation" , "time_domain_expiration" , "qty_nameservers" , 
    "qty_mx_servers" , "ttl_hostname" , "qty_redirects" , "total_dots" , 
    "total_hyphens" , "total_underlines" , "total_commas" , "total_asterisks" , 
    "total_slashes" , "total_equals" , "total_ats" , "total_ands" , "total_percents" , 
    "total_pluses" , "total_tildes" , "domain_in_ip_X1" , "domain_spf_X0" , 
    "domain_spf_X1" , "tls_ssl_certificate_X1" , "url_shortened_X1" , 
    "tld_present_parameters_X0" , "time_domain_activation_avb_X1" , 
    "time_domain_expiration_avb_X1" , "asn_ip_avb_X1" , "qty_ip_resolved_avb_X1" , 
    "qty_redirects_avb_X1" , "directory_length_avb_X1"),
                    listclass=c(""), grupos = 10, sinicio = 1234, repe = 5)

medias5$modelo="AIC"
```

Representación

```{r}
union1<-rbind(medias1, medias2, medias3, medias4, medias5)

par(cex.axis=0.8)
boxplot(data=union1,col="lightblue",tasa~modelo,main="TASA DE FALLOS")


par(cex.axis=0.7)
boxplot(data=union1,col="lightblue",auc~modelo,main="AUC")

# Quitamos BIC para verlo mejor
union1<-rbind(medias1, medias6)

par(cex.axis=0.8)
boxplot(data=union1,col="lightblue",tasa~modelo,main="TASA DE FALLOS")


par(cex.axis=0.7)
boxplot(data=union1,col="lightblue",auc~modelo,main="AUC")

```

# Logit 2

## Repetimos la receta

```{r}
# RECETA
logist_rec <- 
  recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              domain_google_index_avb = forcats::as_factor(ifelse(domain_google_index == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Eliminamos las categorías con <100 obs recategorizando niveles con el nivel modal
  step_mutate(url_google_index = forcats::fct_collapse(url_google_index, 
                                              "0" = c("0", "-1"),
                                              "1" = "1")) |> 
  # Roles
  add_role(where(is.factor), new_role = "qual") |> 
  add_role(where(is.numeric), new_role = "quant") |> 
  # Tratamiento de outliers
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "mad")) > 3 & 
                                                             skewness(x) > 3, NA, x)})) |> 
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "z")) > 2.5 & 
                                                              skewness(x) <= 3, NA, x)})) |> 
  # Imputamos los outliers convetidos a NA
  step_impute_knn(has_role("quant")) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)})) |> 
  # Seleccionamos solo las variables selecciondas
  step_select("qty_tld_url" , "length_url" , "domain_length" , 
    "directory_length" , "file_length" , "params_length" , "asn_ip" , 
    "time_domain_activation" , "time_domain_expiration" , "qty_nameservers" , 
    "qty_mx_servers" , "ttl_hostname" , "qty_redirects" , "total_dots" , 
    "total_hyphens" , "total_underlines" , "total_commas" , "total_asterisks" , 
    "total_slashes" , "total_equals" , "total_ats" , "total_ands" , "total_percents" , 
    "total_pluses" , "total_tildes" , "domain_in_ip_X1" , "domain_spf_X0" , 
    "domain_spf_X1" , "tls_ssl_certificate_X1" , "url_shortened_X1" , 
    "tld_present_parameters_X0" , "time_domain_activation_avb_X1" , 
    "time_domain_expiration_avb_X1" , "asn_ip_avb_X1" , "qty_ip_resolved_avb_X1" , 
    "qty_redirects_avb_X1" , "directory_length_avb_X1", "phishing")
```

corr

```{r}
corr_check <- bake(logist_rec |> prep(), new_data = NULL) |> dplyr::select(-phishing)
```

## Correlación

```{r}
cor_matrix <- 
  corr_check |> dplyr::select(where(is.numeric)) |> cor() |> round(2)
cor_matrix |> corrplot(method = "number", tl.cex = 0.5, number.cex = 0.5, type = "lower")
```

## Flujo y ajuste final

```{r}
set.seed(12345)
# Construimos flujo
logit_flow <-
  workflow() |> add_model(log_reg) |> add_recipe(logist_rec)
# Ajuste
final_logit <- logit_flow |> fit(data = phishing_train)
```

Eval

```{r}
exp(coef(final_logit |> extract_fit_engine()))
final_logit |> extract_fit_engine() |> summary()
glance(final_logit)
pR2(final_logit |> extract_fit_engine())
tidy(final_logit)
```


Predicciones y odds

```{r}
predicciones_logist <-
  augment(final_logit, new_data = phishing_test) |> 
  mutate(odds = .pred_1 / .pred_0, log.odds = log(odds))
predicciones_logist |> conf_mat(phishing, .pred_class)
auc_roc <- predicciones_logist |> roc_auc(truth = phishing, .pred_0)
auc_roc

conf_mat_logit <-
  predicciones_logist |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6") +
  theme_minimal()
conf_mat_logit

roc_par_1 <- 
  predicciones_logist |> roc_curve(truth = phishing, .pred_1)
roc_par_1 |> autoplot() +
   ggtitle("Curva ROC") +
  theme_stata()
```

# Red neuronal

## Apaño

```{r}
# RECETA
logist_rec <- 
  recipe(data = phishing_sample, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params, email_in_url, url_google_index, url_shortened, domain_google_index)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Tratamiento de outliers
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "mad")) > 3 & 
                                                             skewness(x) > 3, NA, x)})) |> 
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "z")) > 2.5 & 
                                                              skewness(x) <= 3, NA, x)})) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)}))
```

bake

```{r}
phishing_sample2 <- 
  bake(logist_rec |> prep(), new_data = NULL)
```

...

```{r}
set.seed(12345)
phishing_split2 <- initial_split(phishing_sample2, strata = phishing, prop = 0.75)
phishing_split2
```

Con el argumento strata le hemos indicado que la partición se haga de forma 
proporcional en función de nuestra variable objetivo, para de este modo tener una 
proporción similar de ambos tipos de webs en cada conjunto generado.

En hoteles split tenemos las instrucciones de la partición, ahora vamos a aplicarlas:

```{r}
set.seed(12345)
phishing_train2 <- training(phishing_split2)
phishing_test2 <- testing(phishing_split2)
```

Tras ello nunca está de más comprobar que efectivamente está hecho de forma 
estratificada

```{r}
# train
phishing_train2 |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))

# test
phishing_test2 |> count(phishing) |> 
  mutate(porc = 100*n/sum(n))
```

Como vemos, se mantienen las proporciones originales para ambos niveles de la 
variable objetivo.

## Validación

Por último crearemos un conjunto de validación, para de esta forma poder obtener 
métricas de la calidad de los modelos que probemos sin ser aplicado directamente 
al conjunto de test. Esto nos permitirá hacer una selección previa de el mejor 
modelo que consideremos aplicar al conjunto de prueba y realizar modelos en 
creciente complejidad.

Para ello, vamos a usar un 30% del 75% de los datos que tenemos en entrenamiento. 
Por lo tanto, nos quedría un 47,5% de los datos en entrenamiento, un 22,5% en 
validación y el 30% de test el cual no va a ser alterado.

```{r}
set.seed(12345)
phishing_val2 <- validation_split(phishing_train2, strata = phishing, prop = 0.7)
phishing_val2
```

Aqui elegimos los parámetros

**Cálculo**: 14 variables, 4406 obs , con 13296 obs por parametro implica 13296/30=443 parametros max.
443/8 = 16 nodos max.

```{r}
variables <- c("qty_tld_url" , "length_url" , "file_length" , 
    "time_response" , "asn_ip" , "time_domain_activation" , "time_domain_expiration" , 
    "qty_ip_resolved" , "qty_nameservers" , "qty_mx_servers" , "ttl_hostname" , 
    "total_dots" , "total_hyphens" , "total_underlines" , "total_commas" , 
    "total_asterisks" , "total_slashes" , "total_equals" , "total_ats" , 
    "total_percents" , "total_pluses" , "domain_in_ip_X1" , "domain_spf_X0" , 
    "tls_ssl_certificate_X1" , "tld_present_parameters_X0" , "tld_present_parameters_X1" , 
    "time_domain_activation_avb_X1" , "qty_redirects_avb_X1" , "directory_length_avb_X1", "phishing")
phishing_prep_final <- phishing_train2 |> dplyr::select(all_of(variables))
```

Paralelizamos

```{r}
library(parallel)
library(doParallel)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

control<-trainControl(method = "cv",
                      number = 10, savePredictions = "all") 
set.seed(12345)
nnetgrid <-  expand.grid(size = c(2, 4, 6, 8, 10, 14), decay = c(0.01, 0.1, 0.001, 0.0001), bag = F)

completo <- data.frame()
listaiter <- c(100, 200, 500, 1000, 2000, 3000, 5000)

for (iter in listaiter)
{
  rednnet<- train(phishing ~ .,
                  data = phishing_prep_final,
                  method = "avNNet",linout = FALSE,maxit = iter,
                  trControl = control, repeats = 5, tuneGrid = nnetgrid, trace = F)
  # Añado la columna del parametro de iteraciones
  rednnet$results$itera <- iter
  # Voy incorporando los resultados a completo
  completo <- rbind(completo, rednnet$results)
  
  
}

stopCluster(make_cluster)
registerDoSEQ()

completo <- completo[order(completo$Accuracy),]

ggplot(completo, aes(x = factor(itera), y = Accuracy, 
                     color=factor(decay), pch=factor(size))) +
  geom_point(position = position_dodge(width = 0.5),size = 3) +
  theme_minimal()
```

1000 iteraciones, 22 nodos input y 0.1 de decay

```{r}
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()
medias5<-cruzadaavnnetbin(data = phishing_prep2,
                       vardep = "phishing",listconti =
                         c("qty_tld_url" , "length_url" , "file_length" , 
    "time_response" , "asn_ip" , "time_domain_activation" , "time_domain_expiration" , 
    "qty_ip_resolved" , "qty_nameservers" , "qty_mx_servers" , "ttl_hostname" , 
    "total_dots" , "total_hyphens" , "total_underlines" , "total_commas" , 
    "total_asterisks" , "total_slashes" , "total_equals" , "total_ats" , 
    "total_percents" , "total_pluses" , "domain_in_ip_X1" , "domain_spf_X0" , 
    "tls_ssl_certificate_X1" , "tld_present_parameters_X0" , "tld_present_parameters_X1" , 
    "time_domain_activation_avb_X1" , "qty_redirects_avb_X1" , "directory_length_avb_X1"),
                       listclass = c(""),grupos = 10,sinicio = 1234, repe = 10, repeticiones = 5, itera = 3000,
                       size = c(10), decay = c(0.1))

stopCluster(make_cluster)
registerDoSEQ()


medias5$modelo = "Red"


union1<-rbind(medias1, medias2, medias3, medias4, medias5)

par(cex.axis = 0.8)
boxplot(data = union1,col = "lightblue",tasa~modelo,main = "TASA DE FALLOS")


par(cex.axis=0.7)
boxplot(data=union1,col="lightblue",auc~modelo,main="AUC")

# Quitamos BIC para verlo mejor
union1<-rbind(medias1, medias2, medias3, medias5)

par(cex.axis = 0.8)
boxplot(data = union1,col = "lightblue",tasa~modelo,main = "TASA DE FALLOS")


par(cex.axis=0.7)
boxplot(data=union1,col="lightblue",auc~modelo,main="AUC")
```

Entrenamos el modelo final de red

```{r}
phishing_prep_final <- phishing_prep_final |> 
  mutate(phishing = ifelse(phishing == "0", "No", "Yes"))
set.seed(12345)
control<-trainControl(method = "cv",
                      number = 10, savePredictions = "all", repeats = 5, classProbs = TRUE) 
nnetgrid <- expand.grid(size = c(10), decay = c(0.1))
nnet_model <- train(phishing ~.,
 method = "nnet", tuneGrid = nnetgrid,
 data = phishing_prep_final, trControl = control, verbose = FALSE, maxit = 3000)

library(pROC)
# Extracción de las predicciones y las observaciones
predictions <- nnet_model$pred$Yes
observations <- ifelse(nnet_model$pred$obs == "Yes", 1, 0)

# Cálculo del AUC
auc <- roc(observations, predictions)$auc
auc


# Cálculo de la sensibilidad
sensitivity <- sum(predictions[observations == 1] >= 0.5) / sum(observations == 1)
sensitivity
# Cálculo de la especificidad
specificity <- sum(predictions[observations == 0] < 0.5) / sum(observations == 0)
specificity

# Cálculo de la varianza
var_auc <- var(roc(observations, predictions)$specificities)
var_auc
var_sensitivity <- var(predictions[observations == 1] >= 0.5)
var_specificity <- var(predictions[observations == 0] < 0.5)

```

Predecimos

```{r}
phishing_test2 <-
  phishing_test2 |> mutate(phishing = ifelse(phishing == "1", "Yes", "No"))
nnet_preds <- predict(nnet_model, phishing_test2)
confusionMatrix(nnet_preds, reference = phishing_test2$phishing, positive = "Yes")

nnet_probs <- predict(nnet_model, phishing_test2, type = "prob")[, 1]
roc_obj <- roc(response = phishing_test2$phishing, nnet_probs)
plot(roc_obj, main = "Curva ROC", xlab = "Tasa de Falsos Positivos", ylab = "Tasa de Verdaderos Positivos")
auc <- auc(roc_obj)
auc
conf_nnet
```



# SVM

## Receta


```{r}
# RECETA
svm_rec <- 
  recipe(data = phishing_train, phishing ~ .) |> 
  # Eliminamos variables
  step_rm(c(qty_slash_domain, qty_questionmark_domain, qty_equal_domain,
            qty_at_domain, qty_and_domain ,qty_exclamation_domain, 
            qty_space_domain, qty_tilde_domain, qty_comma_domain, 
            qty_plus_domain, qty_asterisk_domain, qty_hashtag_domain, 
            qty_dollar_domain, qty_percent_domain, server_client_domain, 
            qty_params, email_in_url, url_google_index, url_shortened, domain_google_index)) |> 
  # Creamos las nuevas variables omitiendo los -1 primero
  step_mutate(across(all_of(contains("_dot_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dots = rowSums(across(all_of(contains("_dot_"))), na.rm = TRUE)) |> 
  step_mutate(across(all_of(contains("_hyphen_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hyphens = rowSums(across(all_of(contains("_hyphen_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_underline_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_underlines = rowSums(across(all_of(contains("_underline_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_comma_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_commas = rowSums(across(all_of(contains("_comma_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_asterisk_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_asterisks = rowSums(across(all_of(contains("_asterisk_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_slash_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_slashes = rowSums(across(all_of(contains("_slash_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_equal_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_equals = rowSums(across(all_of(contains("_equal_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_at_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ats = rowSums(across(all_of(contains("_at_") )), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_and_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_ands = rowSums(across(all_of(contains("_and_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_hashtag_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_hashtags = rowSums(across(all_of(contains("_hashtag_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_space_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_spaces = rowSums(across(all_of(contains("_space_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_dollar_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_dollars = rowSums(across(all_of(contains("_dollar_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_percent_") ), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_percents = rowSums(across(all_of(contains("_percent_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_exclamation_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_exclamations = rowSums(across(all_of(contains("_exclamation"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_questionmark_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_questionmarks = rowSums(across(all_of(contains("_exclamation_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_plus_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_pluses = rowSums(across(all_of(contains("_plus_"))), na.rm = TRUE)) |>
  step_mutate(across(all_of(contains("_tilde_")), ~ifelse(.x == -1, NA, .x))) |> 
  step_mutate(total_tildes = rowSums(across(all_of(contains("_tilde_"))), na.rm = TRUE)) |>
  #Apaño
  step_mutate(tld_present_parameters = tld_present_params) |>
  step_rm(tld_present_params) |> 
  # Reemplazamos los NA's generados por -1 de nuevo.
  step_mutate(across(all_of(contains("params")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("directory")), ~ifelse(is.na(.x), -1, .x))) |>
  step_mutate(across(all_of(contains("file")), ~ifelse(is.na(.x), -1, .x))) |>
  # Eliminamos las variables qty
  step_rm(contains(c("_dot_", "_hyphen_", "_underline_", "_comma_", "_asterisk_", 
                     "_slash_", "_equal_", "_at_", "_and_", "_hashtag_", "_space_", 
                     "_dollar_", "_percent_", "_exclamation_", "_questionmark_", 
                     "_plus_", "_tilde_"))) |>
  # Desglosamos las numéricas con el -1 creando nuevas variables
  step_mutate(time_domain_activation_avb = forcats::as_factor(ifelse(time_domain_activation == -1, 0, 1)),
              time_domain_expiration_avb = forcats::as_factor(ifelse(time_domain_expiration == -1, 0, 1)),
              domain_spf_avb = forcats::as_factor(ifelse(domain_spf == -1, 0, 1)),
              time_response_avb = forcats::as_factor(ifelse(time_response == -1, 0, 1)),
              asn_ip_avb = forcats::as_factor(ifelse(asn_ip == -1, 0, 1)),
              qty_ip_resolved_avb = forcats::as_factor(ifelse(qty_ip_resolved == -1, 0, 1)),
              qty_redirects_avb = forcats::as_factor(ifelse(qty_redirects == -1, 0, 1)),
              tld_present_parameters_avb = forcats::as_factor(ifelse(tld_present_parameters == -1, 0, 1)),
              params_length_avb = forcats::as_factor(ifelse(params_length == -1, 0, 1)),
              directory_length_avb = forcats::as_factor(ifelse(directory_length == -1, 0, 1)),
              file_length_avb = forcats::as_factor(ifelse(file_length == -1, 0, 1))) |> 
  # Tratamiento de outliers
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "mad")) > 3 & 
                                                             skewness(x) > 3, NA, x)})) |> 
  step_mutate(across(all_numeric_predictors(), function(x) {ifelse(length(unique(x)) >= 10 & 
                                                              abs(scores(x, type = "z")) > 2.5 & 
                                                              skewness(x) <= 3, NA, x)})) |> 
  # Filtro de varianza 0
  step_zv(all_predictors()) |> 
  # Dummies
  step_dummy(all_nominal_predictors()) |> 
  # Convertimos a entero
  step_mutate(across(where(is.numeric) & !time_response, function(x) {as.integer(x)})) |>
  # Seleccionamos solo las variables selecciondas
  step_select("qty_tld_url" , "length_url" , "file_length" , 
    "time_response" , "asn_ip" , "time_domain_activation" , "time_domain_expiration" , 
    "qty_ip_resolved" , "qty_nameservers" , "qty_mx_servers" , "ttl_hostname" , 
    "total_dots" , "total_hyphens" , "total_underlines" , "total_commas" , 
    "total_asterisks" , "total_slashes" , "total_equals" , "total_ats" , 
    "total_percents" , "total_pluses" , "domain_in_ip_X1" , "domain_spf_X0" , 
    "tls_ssl_certificate_X1" , "tld_present_parameters_X0" , "tld_present_parameters_X1" , 
    "time_domain_activation_avb_X1" , "qty_redirects_avb_X1" , "directory_length_avb_X1",
    "phishing", skip = TRUE)
```

bake

```{r}
bake(svm_rec |> prep(), new_data = NULL)
```

## Tuneo radial

```{r}
svm_model <- svm_rbf(cost = tune(), rbf_sigma = tune()) |> 
  set_mode("classification") |> 
  set_engine("kernlab")

phishing_svm_wflow <-
  workflow() |> 
  add_recipe(svm_rec) |> 
  add_model(svm_model)
```

Paralelizamos

```{r}
phishing_cv_folds <-
  vfold_cv(data = phishing_train, v = 10, repeats = 5, strata = phishing)
library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

metricas <-
  metric_set(yardstick::accuracy, yardstick::sensitivity, yardstick::specificity, yardstick::roc_auc)
svm_tune_par <- 
  phishing_svm_wflow |> 
  tune_grid(resamples = phishing_cv_folds,
            metrics = metricas,
            control =
              control_grid(verbose = TRUE, allow_par = TRUE, save_pred = TRUE))

stopCluster(make_cluster)
registerDoSEQ()
```

Vemos las métricas

```{r}
svm_tune_par |> collect_metrics()
```

Nos quedamos con el mejor de curva roc

```{r}
svm_tune_par |> show_best("roc_auc")
best_svm <- svm_tune_par |> select_best("roc_auc")
final_svm_flow <- phishing_svm_wflow |> finalize_workflow(best_svm)
final_svm_flow
```

Ajuste final

```{r}
final_svm_fit <-
  final_svm_flow |> last_fit(phishing_split, metrics = metric_set(yardstick::roc_auc, yardstick::specificity, yardstick::sensitivity, yardstick::accuracy))
final_svm_fit |> collect_metrics()
```

Predicciones

```{r}
predict(extract_workflow(final_svm_fit), phishing_test)
predict(extract_workflow(final_svm_fit), phishing_test, type = "prob")
# Inlcuimos predicciones en la tabla
prob_test_svm <-
  augment(extract_workflow(final_svm_fit), phishing_test)

# Matriz de confusión
conf_mat_svm <-
  prob_test_svm |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6")
conf_mat_svm

```

### Plot SVM

```{r}
# Obtener las métricas AUC para cada repetición y conjunto de validación
auc_results_svm <- svm_tune_par |> 
  collect_metrics() |> 
  filter(.metric == "roc_auc") |> 
  dplyr::select(.metric, .estimator, mean, .config)

auc_values_svm <- unlist(auc_results_svm$mean)

# Crear un dataframe con los valores de AUC combinados
df_svm <- data.frame(AUC = auc_values_svm)

# Crear el gráfico de caja
ggplot(df_svm, aes(x = "", y = AUC)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "AUC en conjuntos de validación",
       x = "",
       y = "AUC") +
  theme_minimal()

# ROC
roc_svm <-
  prob_test_svm |> roc_curve(truth = phishing, c(".pred_0"))
roc_svm |> autoplot() +
   ggtitle("Curva ROC") +
  theme_stata()
```



## Tuneo Poli

```{r}
svmp_model <- svm_poly(cost = tune(), degree = tune(), scale_factor = tune()) |> 
  set_mode("classification") |> 
  set_engine("kernlab")

phishing_svmp_wflow <-
  workflow() |> 
  add_recipe(svm_rec) |> 
  add_model(svmp_model)
```

Paralelizamos

```{r}
library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

metricas <-
  metric_set(yardstick::accuracy, yardstick::sensitivity, yardstick::specificity, yardstick::roc_auc)
svmp_tune_par <- 
  phishing_svmp_wflow |> 
  tune_grid(resamples = phishing_cv_folds,
            metrics = metricas,
            control =
              control_grid(verbose = TRUE, allow_par = TRUE, save_pred = TRUE))

stopCluster(make_cluster)
registerDoSEQ()
```

Vemos las métricas

```{r}
svmp_tune_par |> collect_metrics()
```


Nos quedamos con el mejor de curva roc

```{r}
svmp_tune_par |> show_best("roc_auc")
best_svmp <- svmp_tune_par |> select_best("roc_auc")
final_svmp_flow <- phishing_svmp_wflow |> finalize_workflow(best_svmp)
final_svmp_flow
```

Ajuste final

```{r}
final_svmp_fit <-
  final_svmp_flow |> last_fit(phishing_split, metrics = metric_set(yardstick::accuracy, yardstick::sensitivity, yardstick::specificity, yardstick::roc_auc))
final_svmp_fit |> collect_metrics()
```

Predicciones

```{r}
predict(extract_workflow(final_svmp_fit), phishing_test)
predict(extract_workflow(final_svmp_fit), phishing_test, type = "prob")
# Inlcuimos predicciones en la tabla
prob_test_svmp <-
  augment(extract_workflow(final_svmp_fit), phishing_test)

# Matriz de confusión
conf_mat_svmp <-
  prob_test_svmp |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6")
conf_mat_svmp
```

## Tuneo lineal

```{r}
svml_model <- svm_linear(cost = tune(), margin = tune()) |> 
  set_mode("classification") |> 
  set_engine("kernlab")

phishing_svml_wflow <-
  workflow() |> 
  add_recipe(svm_rec) |> 
  add_model(svml_model)
```

Paralelizamos

```{r}
library(parallel)
library(doParallel)
set.seed(12345)
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()

metricas <-
  metric_set(yardstick::accuracy, yardstick::sensitivity, yardstick::specificity, yardstick::roc_auc)
svml_tune_par <- 
  phishing_svml_wflow |> 
  tune_grid(resamples = phishing_cv_folds,
            metrics = metricas,
            control =
              control_grid(verbose = TRUE, allow_par = TRUE, save_pred = TRUE))

stopCluster(make_cluster)
registerDoSEQ()
```

Vemos las métricas

```{r}
svml_tune_par |> collect_metrics()
```


Nos quedamos con el mejor de curva roc

```{r}
svml_tune_par |> show_best("roc_auc")
best_svml <- svml_tune_par |> select_best("roc_auc")
final_svml_flow <- phishing_svml_wflow |> finalize_workflow(best_svml)
final_svml_flow
```

Ajuste final

```{r}
final_svml_fit <-
  final_svml_flow |> last_fit(phishing_split, metrics = metric_set(roc_auc))
final_svml_fit |> collect_metrics()
```

Predicciones

```{r}
predict(extract_workflow(final_svml_fit), phishing_test)
predict(extract_workflow(final_svmp_fit), phishing_test, type = "prob")
# Inlcuimos predicciones en la tabla
prob_test_svml <-
  augment(extract_workflow(final_svml_fit), phishing_test)

# Matriz de confusión
conf_mat_svml <-
  prob_test_svml |> 
  conf_mat(truth = phishing, estimate = .pred_class) |> 
  autoplot(type = "heatmap") +
  theme_gdocs() +
   scale_fill_gradient(high = "#00008B", low = "#ADD8E6")
conf_mat_svml
```


